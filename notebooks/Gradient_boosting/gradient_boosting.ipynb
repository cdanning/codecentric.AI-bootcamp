{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codecentric.AI Bootcamp - Gradient Boosting & XGBoost\n",
    "\n",
    "Hallo und herzlich Willkommen zurück beim **codecentric.AI Bootcamp**.\n",
    "\n",
    "Heute geht es wieder um die Grundlagen des maschinellen Lernens, nämlich um **Gradient Boosting** und **XGBoost**. Dieses Notebook enthält Beispiele und Übungsaufgaben.\n",
    "\n",
    "Eine theoretische Einführung in Gradient Boosting gibt es in diesem [YouTube video](https://youtu.be/xXZeVKP74ao)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850\"\n",
       "            height=\"650\"\n",
       "            src=\"https://www.youtube.com/embed/xXZeVKP74ao\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f702c407240>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lade Video\n",
    "from IPython.display import IFrame    \n",
    "IFrame('https://www.youtube.com/embed/xXZeVKP74ao', width=850, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu diesem Notebook gibt es ebenfalls ein [Video](https://youtu.be/4oLsev95Lh4), in dem ich euch durch dieses Beispiel durchführe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lade Video\n",
    "from IPython.display import IFrame    \n",
    "IFrame('https://www.youtube.com/embed/4oLsev95Lh4', width=850, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotheken\n",
    "\n",
    "Zunächst laden wir die grundlegenden Pakete, die wir für die Vorbereitung der Daten benötigten. Dazu gehören\n",
    "\n",
    "- **numpy**: NumPy ist das wichtigste Paket für maschinelles Lernen in Python, denn es bietet die nötigen Funktionen für die Arbeit mit Matrizen und n-dimensionalen Arrays, linearer Algebra, und mehr.\n",
    "- **pandas**: pandas erleichtert das Arbeiten mit Daten in Python.\n",
    "- **matplotlib**: Zum Erstellen von Graphiken und Abbildungen aus unseren Daten nutzen wir matplotlib. Den zusätzlichen Befehl `matplotlib inline` geben wir in unserem Juypter Notebook mit, damit wir die generierten Plots unterhalb des Code-Chunks sehen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einlesen und vorbereiten\n",
    "\n",
    "Genau wie [Random Forest](https://www.youtube.com/embed/ieF_QjVUNEQ), ist Gradient Boosting einer von vielen **Machine Learning** Algorithmen des **überwachten Lernens**, im Englischen \"supervised learning\" genannt.\n",
    "\n",
    "Darum nutzen wir hier denselben Datensatz, wie in dem [Random Forest](https://www.youtube.com/embed/ieF_QjVUNEQ) Kapitel.\n",
    "\n",
    "Bei überwachtem Lernen nutzen wir sogenannte gelabelte **Trainingsdaten**.\n",
    "Das bedeutet, dass wir für jeden unseren Datenpunkte ein bekanntes Ergebnis als\n",
    "Zielgröße haben. Die Maschine lernt nun also dieses bekannte Ergebnis möglichst gut mit\n",
    "den vorhandenen Daten abzubilden - sie lernt quasi die optimale mathematische \n",
    "Repräsentation der Daten um mit möglichst hoher Genauigkeit auf die Zielgröße zu kommen.\n",
    "Die gelernte mathematische Repräsentation kann dann auf neue - ungelabelte - Daten\n",
    "angewandt werden und so für Vorhersagen genutzt werden.\n",
    "\n",
    "Das Scikit-learn Paket beinhaltet eine Reihe von **Datensätzen**, die wir direkt einladen und nutzen können. Eine Übersicht über die enthaltenen Datensätze ist hier zu finden: http://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "Hier wollen wir einen Datensatz über **Weinqualität** nutzen, um zu zeigen, wie man mit Scikit-learn Random Forest Modelle trainiert. Dafür laden wir zunächst das Paket `sklearn.datasets` und daraus die `load_wine` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `load_wine` Funktion gehört zu den \"dataset loaders\" und wird genutzt, um kleinere Datensätze (wie unseren hier) zu laden. Wenn wir `return_X_y=True` setzen, bekommen wir zwei getrennte Objekte zurück:\n",
    "\n",
    "1. Nur die Feature\n",
    "2. Nur Target/Antwortvariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings- und Testsets\n",
    "\n",
    "Als nächstes wollen wir unsere Daten in **Trainings- und Testsets** aufteilen. Dafür gibt es in `sklearn.model_selection` die `train_test_split` Funktion. Die laden wir wieder ein und geben ihr folgende Argumente mit:\n",
    "\n",
    "- `features`: die Featurewerte\n",
    "- `target`: die Antwortvariable\n",
    "- `test_size`: wie viele der Daten in das Testset sollen (hier 30%)\n",
    "- `random_state`: Seed für die Pseudozufallsgenerierung von Nummern\n",
    "- `stratify`: optional, hier sollen Trainings- und Testset die gleichen Proportionen der Klassenverteilungen in unserer Antwortvariablen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    target,\n",
    "                                                    test_size = 0.30,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Output von `train_test_split` sind vier Objekte:\n",
    "\n",
    "- `X_train`: Featurewerte für alle Trainingsinstanzen\n",
    "- `X_test`: Featurewerte für alle Testinstanzen\n",
    "- `y_train`: Label/Klassen für alle Trainingsinstanzen\n",
    "- `y_test`: Label/Klassen für alle Testinstanzen\n",
    "\n",
    "## Gradient Boosting mit Entscheidungsbäumen\n",
    "\n",
    "In der vorhergehenden Lektion ging es bereits um Random Forests, eine Methode, die viele Ähnlichkeiten zu Gradient Boosting hat. Denn auch Gradient Boosting wird für überwachtes Machine Learning verwendet, und ist also - wie Random Forests, neuronale Netze und andere Algorithmen - für **Klassifikations- & Regressionsaufgaben** geeignet.\n",
    "\n",
    "Genau wie Random Forests sind Gradient Boosting Modelle **Ensemble Lerner**, d.h. sie setzen sich aus vielen einzelnen Modellen zusammen - und in der Regel sind diese einzelnen Modelle ebenfalls **Entscheidungsbäume**.\n",
    "\n",
    "Das Grundprinzip ist also das Gleiche wie bei Random Forests: während einzelne Entscheidungsbäume keine besonders gute Vorhersagekraft haben und schnell zum Overfitting neigen, ist ein Ensemble aus mehreren Entscheidungsbäumen deutlich besser!\n",
    "\n",
    "Eine detaillierte Erklärung darüber, was Entscheidungsbäume sind und wie sie funktionieren findet ihr in dem oben erwähnten [Kapitel zu Random Forests](https://www.youtube.com/embed/W0ZcjQFTF_g). Darum hier nur kurz die wichtigsten Punkte: Entscheidungsbäume bestehen aus **Knoten oder Ästen**, an denen die **Feature** aufgeteilt werden und die Instanzen entsprechend dieser Aufteilung den Baum **hinunter wandern**. \n",
    "\n",
    "Wir haben also eine Kette von **wenn... dann...** Entscheidungen, die unsere Daten immer weiter differenzieren. Diese Differenzierung passiert so lange, bis wir an einem Endpunkt oder **Blatt** angekommen sind. Diese Endpunkte stellen das **Ergebnis**, also bei einer Klassifikation die vorhergesagte Klasse, dar.\n",
    "\n",
    "Solche Entscheidungsbäume dienen für das Gradient Boosting nun als **Basis-Modell**.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Der Name \"Gradient Boosting\" beschreibt schon die wesentlichen Funktionen, mit denen diese Methode lernt. Gucken wir uns zunächst das **Boosting** an, mit dem wir einzelne Entscheidungsbäume kombinieren können. \n",
    "\n",
    "Im Gegensatz zum Bagging, das bei Random Forests verwendet wird, werden die einzelnen Entscheidungsbäume beim Boosting nicht auf zufälligen Teilmengen der Daten trainiert, sondern auf **gewichteten Teilmengen**. Nach jeder Trainingsrunde, bei der ein Entscheidungsbaum trainiert wurde, werden die Instanzen, deren Vorhersage falsch war, entsprechend des Vorhersagefehlers stärker gewichtet als richtig vorhergesagte Instanzen. Die Daten mit höheren Gewichten, werden entsprechend bevorzugt für die nächsten Teilmengen gesampelt. Die Idee ist, dass **schwierige Fälle besonders genau gelernt werden müssen** und deshalb häufiger gesampelt werden sollen als einfache Fälle.\n",
    "\n",
    "Das Trainieren auf Teilmengen bezeichnen wir auch als **stochastisches Gradient Boosting**, und hilft dabei, Overfitting zu vermeiden und eine bessere Generalisierbarkeit unseres Modells zu erreichen.\n",
    "\n",
    "Boosting funktioniert also wie folgt:\n",
    "\n",
    "1. Wir haben ein **Basis-Modell**, in der Regel Entscheidungsbäume.\n",
    "2. Mit diesem Basis-Modell machen wir **Vorhersagen** auf unseren Trainingsdaten.\n",
    "3. Die **Gewichtung** der Instanzen erfolgt nun basierend auf den Vorhersagen. Ist die Vorhersage richtig, bekommt die Instanz ein niedrigeres Gewicht. Ist die Vorhersage falsch, bekommt die Instanz ein höheres Gewicht.\n",
    "4. Mit diesen Gewichten wird nun eine neue Teilmenge der Daten gesampelt und ein **neuer Entscheidungsbaum wird trainiert**.\n",
    "\n",
    "## Gradientenoptimierung\n",
    "\n",
    "Der Gradient kommt nun für die Optimierung ins Spiel. Lernen durch Optimierung mit Gradienten ist uns bereits in der Lektion zu [Neuronalen Netzen und Deep Learning begegnet](https://www.youtube.com/embed/Z42fE0MGoDQ).\n",
    "\n",
    "Die wichtigsten Aspekte daraus waren, dass wir mit einem neuronalen Netz ein Ergebnis oder eine Vorhersage berechnet haben, die wir mit dem tatsächlichen Ergebnis abgeglichen haben. Aus der **Differenz zwischen Vorhersage und Wirklichkeit** ergab sich ein Fehler, den wir mit einer **Loss-Funktion** dargestellt haben. Diese Loss-Funktion sollte während des Trainingsprozesses **minimiert** werden. Gradienten kamen dabei während dieser Minimierung ins Spiel. Die Methode, die wir uns dabei genauer angesehen haben, hieß **Gradientenabstieg** oder Gradient Descent. Der Gradient ist die **partielle Ableitung** unserer Loss-Funktion und beschreibt also die Fehlerlandschaft. Dort, wo der Gradient am steilsten ist, können wir in der nächsten Trainingsrunde die größte Fehlerminimierung erreichen, das Neuronale Netz lernt also, indem es den Gradienten \"hinab steigt\".\n",
    "\n",
    "Mehr dazu in unserem [Video zu Neuronalen Netzen auf YouTube](https://www.youtube.com/embed/Z42fE0MGoDQ).\n",
    "\n",
    "Gradient Boosting nutzt Gradienten in einer ähnlichen Art wie neuronale Netze, um ebenfalls eine Loss-Funktion zu minimieren. Der große Unterschied ist, dass neuronale Netze den Fehler in einem Modell minimieren, während Gradient Boosting den Fehlern von einem **Ensemble aus Basis-Modellen** minmieren will.\n",
    "Auch hier nutzen wir unser Basis-Modell zunächst für Vorhersagen auf dem Trainingsset und gleichen diese mit der Wirklichkeit ab, woraus sich der Fehler und der Loss ergibt.\n",
    "Der Gradient dieser Loss-Funktion wird nun allerdings **zu dem bestehenden Trainingsprozess hinzugefügt**, so dass der nächste Entscheidungsbaum zusätzlich auf diese Gradienten gefittet wird.\n",
    "\n",
    "## Scikit-learn\n",
    "\n",
    "Wie trainieren wir nun Gradient Boosting Modelle in Python?\n",
    "\n",
    "Dafür gibt es wieder mehrere Möglichkeiten und Pakete. Analog zu Random Forests, stelle ich in den Jupyter-Notebooks mit Beispielen, Aufgaben und Lösungen zum einen Scikit-learn vor.\n",
    "\n",
    "[Scikit-learn](http://scikit-learn.org/stable/) ist eine Machine Learning Bibliothek für Python, die das Trainieren von vielen verschiedenen Algorithmen für Klassifikation, Regression, Clustering, und mehr sehr einfach macht. In Scikit-learn können wir den [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) oder [Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "\n",
    "Da wir mit Entscheidungsbaum-Ensembles und mit Gradienten arbeiten, finden wir in den Hyperparametern, die unseren Lernprozess beschreiben, viele alte Bekannte aus Random Forests und Neuronalen Netzen wieder. \n",
    "\n",
    "- Das ist zum einen die Lernrate, also die Schrittgröße für den Gradientenabstieg, und die Schrumpfung oder Shrinkage, mit der wir die Lernrate definieren können.\n",
    "- Außerdem können wir natürlich die Loss-Funktion wählen,\n",
    "- die Anzahl an Bäumen für unser Ensemble,\n",
    "- sowie die Anzahl an Instanzen, die in jedem Blatt vorhanden sein müssen\n",
    "- und die Baumtiefe und -komplexität.\n",
    "- Für das Boosting definieren wir den Anteil der gesampelten Instanzen\n",
    "- und den Anteil der verwendeten Feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=3, min_samples_split=3,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              n_iter_no_change=None, presort='auto', random_state=42,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    loss = \"deviance\",\n",
    "    n_estimators = 200,\n",
    "    min_samples_split = 3,\n",
    "    min_samples_leaf = 3,\n",
    "    max_depth = 3,\n",
    "    random_state = 42\n",
    ")\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie zuvor können wir die `predict` Funktion verwenden, um unsere Testdaten klassifizieren zu lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluieren können wir diese Vorhersagen zum Beispiel mit einer Kreuzmatrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:0</th>\n",
       "      <th>pred:1</th>\n",
       "      <th>pred:2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true:0</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true:1</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true:2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred:0  pred:1  pred:2\n",
       "true:0      18       0       0\n",
       "true:1       1      20       0\n",
       "true:2       0       0      15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "unique_label = np.unique(y_test)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred, labels=unique_label), \n",
    "                   index=['true:{:}'.format(x) for x in unique_label], \n",
    "                   columns=['pred:{:}'.format(x) for x in unique_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativ zu vorhergesagten Klassen können wir uns mit der `predict_proba` Funktion Vorhersagewahrscheinlichkeiten ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99868601e-01, 5.74801819e-05, 7.39191195e-05],\n",
       "       [4.34978317e-05, 9.99878647e-01, 7.78552701e-05],\n",
       "       [9.99787169e-01, 9.72797236e-05, 1.15551236e-04],\n",
       "       [9.99870065e-01, 5.68394980e-05, 7.30952045e-05],\n",
       "       [9.97155955e-01, 2.76411783e-03, 7.99275452e-05],\n",
       "       [9.99819851e-01, 1.00206341e-04, 7.99421802e-05],\n",
       "       [1.35093150e-04, 6.52984404e-05, 9.99799608e-01],\n",
       "       [4.64357093e-05, 9.99908393e-01, 4.51716904e-05],\n",
       "       [5.47128857e-01, 4.21140331e-01, 3.17308125e-02],\n",
       "       [9.24970432e-05, 3.18824100e-03, 9.96719262e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.predict_proba(X_test)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Eine spezielle Implementierung der Gradient Boosting Methode möchte ich hier noch genauer vorstellen: das sogenannte **Extreme Gradient Boosting oder XGBoost**. XGBoost ist besonders beliebt und bekannt worden, weil dieser Algorithmus so gut ist, dass er in vielen Kaggle Competitions zum Sieg geführt hat. XGBoost ist deshalb so erfolgreich, weil er ein paar geniale Tricks verwendet, die das Gradienten Boosting noch effektiver machen.\n",
    "\n",
    "Einer dieser Tricks ist, dass statt der ersten partiellen Ableitung, die **Gradienten zweiter Ordnung** der Loss-Funktion verwendet werden. Dadurch erhält unser Modell bessere Informationen über die Richtung und die Größe, mit der es den Gradienten hinabsteigen muss, um das Minimum der Loss-Funktion zu finden und die besten Entscheidungsbäume zu approximieren.\n",
    "\n",
    "XGBoost verwendet außerdem **L1 und L2 Regularisierung**, wodurch es besser in der Lage ist zu generalisieren und Overfitting zu vermeiden.\n",
    "\n",
    "Außerdem ist XGBoost extrem schnell und kann zusätzlich parallelisiert werden und für verteiltes Trainieren, z.B. auf Spark Clustern verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost können wir in Python mit dem `xgboost` Paket verwenden. Und zwar entweder als Scikit-learn Implementierung oder nativ.\n",
    "\n",
    "### Scikit-learn-Implementierung\n",
    "\n",
    "Für Scikit-learn sieht der Workflow ähnlich aus wie oben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:0</th>\n",
       "      <th>pred:1</th>\n",
       "      <th>pred:2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true:0</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true:1</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true:2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred:0  pred:1  pred:2\n",
       "true:0      18       0       0\n",
       "true:1       0      21       0\n",
       "true:2       0       0      15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xgboost.predict(X_test)\n",
    "unique_label = np.unique(y_test)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred, labels=unique_label), \n",
    "                   index=['true:{:}'.format(x) for x in unique_label], \n",
    "                   columns=['pred:{:}'.format(x) for x in unique_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost-nativ\n",
    "\n",
    "Das [XGBoost Python-Paket](https://xgboost.readthedocs.io/en/latest/python/index.html) arbeitet mit einer speziellen Datenstruktur, der **DMatrix**. Deshalb wandeln wir unsere Trainings- und Testdaten zunächst in dieses Format um:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(data=X_test,label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die einfachste Art mit `xgboost` zu arbeiten, ist die `train` Funktion. Wir definieren [Hyperparameter](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters) und füttern sie in die, zusammen mit der DMatrix und der Anzahl an Boosting-Runden in die `train` Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta':0.1, \n",
    "         'max_depth':5,\n",
    "         'silent':1}\n",
    "\n",
    "bst = xgb.train(dtrain=data_dmatrix, \n",
    "                params=params, \n",
    "                num_boost_round=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns während des Trainings die Fehlerwerte ausgeben lassen (hier für Trainings- und Validierungsdaten), indem wir eine **Watchlist** definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.79895\ttest-rmse:0.814\n",
      "[1]\ttrain-rmse:0.72173\ttest-rmse:0.739222\n",
      "[2]\ttrain-rmse:0.651991\ttest-rmse:0.672404\n",
      "[3]\ttrain-rmse:0.589008\ttest-rmse:0.612833\n",
      "[4]\ttrain-rmse:0.532125\ttest-rmse:0.559867\n",
      "[5]\ttrain-rmse:0.480752\ttest-rmse:0.512924\n",
      "[6]\ttrain-rmse:0.434354\ttest-rmse:0.471473\n",
      "[7]\ttrain-rmse:0.392449\ttest-rmse:0.435031\n",
      "[8]\ttrain-rmse:0.354602\ttest-rmse:0.403149\n",
      "[9]\ttrain-rmse:0.320419\ttest-rmse:0.37541\n",
      "[10]\ttrain-rmse:0.289545\ttest-rmse:0.351425\n",
      "[11]\ttrain-rmse:0.261648\ttest-rmse:0.331023\n",
      "[12]\ttrain-rmse:0.236439\ttest-rmse:0.313425\n",
      "[13]\ttrain-rmse:0.213722\ttest-rmse:0.297446\n",
      "[14]\ttrain-rmse:0.193186\ttest-rmse:0.283696\n",
      "[15]\ttrain-rmse:0.174585\ttest-rmse:0.273047\n",
      "[16]\ttrain-rmse:0.157831\ttest-rmse:0.263371\n",
      "[17]\ttrain-rmse:0.142691\ttest-rmse:0.255207\n",
      "[18]\ttrain-rmse:0.129019\ttest-rmse:0.248442\n",
      "[19]\ttrain-rmse:0.116665\ttest-rmse:0.242927\n",
      "[20]\ttrain-rmse:0.105506\ttest-rmse:0.238074\n",
      "[21]\ttrain-rmse:0.095428\ttest-rmse:0.234123\n",
      "[22]\ttrain-rmse:0.08632\ttest-rmse:0.230853\n",
      "[23]\ttrain-rmse:0.078108\ttest-rmse:0.228242\n",
      "[24]\ttrain-rmse:0.070689\ttest-rmse:0.226081\n",
      "[25]\ttrain-rmse:0.063983\ttest-rmse:0.22437\n",
      "[26]\ttrain-rmse:0.057922\ttest-rmse:0.22283\n",
      "[27]\ttrain-rmse:0.052427\ttest-rmse:0.221605\n",
      "[28]\ttrain-rmse:0.047461\ttest-rmse:0.220616\n",
      "[29]\ttrain-rmse:0.042971\ttest-rmse:0.219827\n",
      "[30]\ttrain-rmse:0.03892\ttest-rmse:0.218684\n",
      "[31]\ttrain-rmse:0.035265\ttest-rmse:0.217608\n",
      "[32]\ttrain-rmse:0.031965\ttest-rmse:0.216683\n",
      "[33]\ttrain-rmse:0.028967\ttest-rmse:0.216142\n",
      "[34]\ttrain-rmse:0.026279\ttest-rmse:0.215445\n",
      "[35]\ttrain-rmse:0.023837\ttest-rmse:0.215088\n",
      "[36]\ttrain-rmse:0.02162\ttest-rmse:0.214807\n",
      "[37]\ttrain-rmse:0.019648\ttest-rmse:0.214367\n",
      "[38]\ttrain-rmse:0.017848\ttest-rmse:0.214064\n",
      "[39]\ttrain-rmse:0.01621\ttest-rmse:0.2139\n",
      "[40]\ttrain-rmse:0.014731\ttest-rmse:0.213658\n",
      "[41]\ttrain-rmse:0.013392\ttest-rmse:0.213545\n",
      "[42]\ttrain-rmse:0.012181\ttest-rmse:0.213451\n",
      "[43]\ttrain-rmse:0.011086\ttest-rmse:0.213355\n",
      "[44]\ttrain-rmse:0.010097\ttest-rmse:0.213273\n",
      "[45]\ttrain-rmse:0.009192\ttest-rmse:0.213153\n",
      "[46]\ttrain-rmse:0.008384\ttest-rmse:0.213032\n",
      "[47]\ttrain-rmse:0.007649\ttest-rmse:0.212981\n",
      "[48]\ttrain-rmse:0.006984\ttest-rmse:0.212893\n",
      "[49]\ttrain-rmse:0.006384\ttest-rmse:0.212827\n"
     ]
    }
   ],
   "source": [
    "watchlist = [(data_dmatrix, 'train'), (test_dmatrix, 'test')]\n",
    "bst = xgb.train(dtrain=data_dmatrix, \n",
    "                params=params, \n",
    "                num_boost_round=50,\n",
    "                evals=watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativ können wir die `cv` Funktion verwenden, um Kreuzvalidierung zu nutzen, hier 3-Fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_cv = xgb.cv(dtrain=data_dmatrix, \n",
    "                params=params, \n",
    "                nfold=3,\n",
    "                num_boost_round=50,\n",
    "                early_stopping_rounds=10,\n",
    "                metrics=\"rmse\", \n",
    "                as_pandas=True, \n",
    "                seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier im Vergleich Trainings- und Validierungs-RMSE (Root Mean Squared Error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.010474</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.341533</td>\n",
       "      <td>0.119269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.009609</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.341523</td>\n",
       "      <td>0.119392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.008820</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.341497</td>\n",
       "      <td>0.119514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.341515</td>\n",
       "      <td>0.119564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.007447</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.341505</td>\n",
       "      <td>0.119599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "45         0.010474        0.001088        0.341533       0.119269\n",
       "46         0.009609        0.001079        0.341523       0.119392\n",
       "47         0.008820        0.001065        0.341497       0.119514\n",
       "48         0.008101        0.001049        0.341515       0.119564\n",
       "49         0.007447        0.001032        0.341505       0.119599"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_cv.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluieren können wir unser Modell auf den Testdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0]\\teval-rmse:0.212827'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.eval(test_dmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oder wir nutzen es für Vorhersagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6844833e-03, 9.9383903e-01, 2.6844833e-03, 2.6844833e-03,\n",
       "       2.4542863e-02, 2.6844833e-03, 1.9906461e+00, 9.9727887e-01,\n",
       "       9.6621418e-01, 1.9906461e+00, 9.9603283e-01, 9.9666268e-01,\n",
       "       1.9906461e+00, 9.9865365e-01, 5.5614077e-03, 1.9906461e+00,\n",
       "       8.5669547e-01, 9.6929204e-01, 1.9906461e+00, 1.9906461e+00,\n",
       "       9.9789667e-01, 1.9906461e+00, 1.9906461e+00, 1.9906461e+00,\n",
       "       9.9319249e-01, 1.9906461e+00, 2.6844833e-03, 2.5594944e-01,\n",
       "       2.6844833e-03, 9.5309651e-01, 9.5150578e-01, 8.5669547e-01,\n",
       "       1.9906461e+00, 1.0012242e+00, 9.7202969e-01, 1.9906461e+00,\n",
       "       9.9319249e-01, 9.9772471e-01, 1.0010464e+00, 2.6844833e-03,\n",
       "       1.9906461e+00, 2.6844833e-03, 2.6844833e-03, 2.6844833e-03,\n",
       "       2.6844833e-03, 9.9543166e-01, 9.9707168e-01, 2.6844833e-03,\n",
       "       1.9906461e+00, 1.3731916e-03, 9.9782139e-01, 9.9860030e-01,\n",
       "       1.9906461e+00, 2.6844833e-03], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_xgb = bst.predict(test_dmatrix)\n",
    "preds_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 2., 1., 1., 2., 1., 1., 2., 1., 0., 2., 1.,\n",
       "       0., 2., 2., 1., 2., 2., 2., 1., 2., 0., 1., 0., 1., 0., 1., 2., 1.,\n",
       "       1., 2., 1., 1., 1., 0., 2., 0., 0., 0., 0., 1., 1., 0., 2., 0., 1.,\n",
       "       1., 2., 0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = test_dmatrix.get_label()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting mit H2O\n",
    "\n",
    "Auch in H2O finden wir eine Implementierung für Gradient Boosting. [H2O](https://www.h2o.ai/) ist eine beliebte Open-Source Machine Learning Plattform, mit der man ebenfalls sehr einfach und schnell Random Forests trainieren und tunen kann.\n",
    "\n",
    "Im Unterschied zu Scikit-learn läuft H2O auf Clustern mit einem Java Backend. Wir können unser Cluster lokal starten oder z.B. mit Spark, Hadoop oder anderen Umgebungen. Je nachdem, wie rechenintensiv unsere Modelle sind, können wir durch das Nutzen von mehreren Clustern und Knoten eine deutlich schnellere Trainingszeit erreichen als mit Scikit-learn.\n",
    "\n",
    "Zunächst importieren wir die `h2o` Bibliothek und starten das Cluster. Mit `nthreads = -1` werden alle Kerne einer Machine genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_181\"; OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-2~deb9u1-b13); OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n",
      "  Starting server from /usr/local/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpya5c3rm9\n",
      "  JVM stdout: /tmp/tmpya5c3rm9/h2o_unknownUser_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpya5c3rm9/h2o_unknownUser_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>7 days and 3 hours </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_unknownUser_3k9t60</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>444.5 Mb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.0.2\n",
       "H2O cluster version age:    7 days and 3 hours\n",
       "H2O cluster name:           H2O_from_python_unknownUser_3k9t60\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    444.5 Mb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.7 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the H2O library and start up the H2O cluster locally on your machine\n",
    "import h2o\n",
    "\n",
    "# Number of threads, nthreads = -1, means use all cores on your machine\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  \\\n",
       "0      0    14.23        1.71  2.43               15.6      127.0   \n",
       "1      0    13.20        1.78  2.14               11.2      100.0   \n",
       "2      0    13.16        2.36  2.67               18.6      101.0   \n",
       "3      0    14.37        1.95  2.50               16.8      113.0   \n",
       "4      0    13.24        2.59  2.87               21.0      118.0   \n",
       "\n",
       "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   color_intensity   hue  od280/od315_of_diluted_wines  proline  \n",
       "0             5.64  1.04                          3.92   1065.0  \n",
       "1             4.38  1.05                          3.40   1050.0  \n",
       "2             5.68  1.03                          3.17   1185.0  \n",
       "3             7.80  0.86                          3.45   1480.0  \n",
       "4             4.32  1.04                          2.93    735.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_wine()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "target = pd.DataFrame({'class':data.target})\n",
    "\n",
    "df_c = pd.concat([target, df], axis=1)\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir mit `h2o` arbeiten können, müssen wir unsere Daten in ein H2O Frame konvertieren. Dafür gibt es die `h2o.H2OFrame` Funktion. Unsere Antwortvariable ist im Moment noch numerisch, h2o würde also eine Regression durchführen. Wir wollen aber eine Klassifikation trainieren, darum konvertiere ich die Variable mit `asfactor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf = h2o.H2OFrame(df_c)\n",
    "hf[0] = hf[0].asfactor()  \n",
    "hf[0].isfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch in `h2o` können wir unsere Daten sehr einfach in Trainings- und Testsets einteilen. Zu Demonstrationszwecken teile ich die Daten hier in drei Sets ein: Training, Validierung und Test. Bei der geringen Anzahl an Instanzen ist das aber eigentlich nicht sinnvoll - wir würden stattdessen Kreuzvalidierung verwenden.\n",
    "\n",
    "Die `split_frame` Funktion bekommt einen Vektor von Werten zwischen 0 und 1, die in Summe weniger als 1 sein müssen. Diese Werte stellen den Anteil der Daten für das Trainingsset (hier 70%), das Validierungsset (hier 15%) und das Testset (die verbleibenden 15%) dar. Die drei Datensets bekommen wir als separate Objekte zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = hf.split_frame([0.7, 0.15], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Vorbereitung definieren wir nun auch noch einen Vektor mit allen Featurenamen (Spaltennamen), die wir zum Trainieren verwenden wollen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_X = hf.col_names[1:len(hf.col_names)]\n",
    "hf_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Außerdem generieren wir noch einen mit der Antwortvariablen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_y = hf.col_names[0]\n",
    "hf_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die entsprechende Funktion hier heißt [H2O Gradient Boosting Estimator](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_h2o = H2OGradientBoostingEstimator(\n",
    "    max_depth = 3,\n",
    "    min_rows = 3,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `train` erfolgt dann das eigentliche Traineren des Modells. Wenn wir mit einem Validierungsset arbeiten, wie hier, geben wir dieses neben dem Trainingsset an. Falls wir kein Validierungsset hätten und Kreuzvalidierung durchführen wollten, würden wir statt `validation_frame` die Argumente `nfolds` (Anzahl Folds) im H2ORandomForestEstimator setzen. Die Namen der Spalten für Feature und Antwortvariablen geben wir mit `x` und `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "gbm_h2o.train(x = hf_X, \n",
    "         y = hf_y, \n",
    "         training_frame = train, \n",
    "         validation_frame = valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Übersicht über die Performance unseres Modells auf den Testdaten erhalten wir mit der `model_performance` Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsMultinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.024284013580955083\n",
      "RMSE: 0.1558332877820239\n",
      "LogLoss: 0.0688345169342984\n",
      "Mean Per-Class Error: 0.06666666666666667\n",
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>2</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>9.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0 / 9</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>10.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0 / 10</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td>\n",
       "<td>0.2</td>\n",
       "<td>1 / 5</td></tr>\n",
       "<tr><td>9.0</td>\n",
       "<td>11.0</td>\n",
       "<td>4.0</td>\n",
       "<td>0.0416667</td>\n",
       "<td>1 / 24</td></tr></table></div>"
      ],
      "text/plain": [
       "0    1    2    Error      Rate\n",
       "---  ---  ---  ---------  ------\n",
       "9    0    0    0          0 / 9\n",
       "0    10   0    0          0 / 10\n",
       "0    1    4    0.2        1 / 5\n",
       "9    11   4    0.0416667  1 / 24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Hit Ratios: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>k</b></td>\n",
       "<td><b>hit_ratio</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.9583333</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "k    hit_ratio\n",
       "---  -----------\n",
       "1    0.958333\n",
       "2    1\n",
       "3    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "performance = gbm_h2o.model_performance(test_data=test)\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost mit H2O\n",
    "\n",
    "Auch in H2O finden wir eine Implementierung von XGBoost.\n",
    "\n",
    "Hier heißt die Funktion entsprechend [H2O XGBoost Estimator](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_h2o = H2OXGBoostEstimator(\n",
    "    max_depth = 3,\n",
    "    min_rows = 3,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "xgb_h2o.train(x = hf_X, \n",
    "         y = hf_y, \n",
    "         training_frame = train, \n",
    "         validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsMultinomial: xgboost\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.030792554899798635\n",
      "RMSE: 0.17547807526810474\n",
      "LogLoss: 0.11250859163847905\n",
      "Mean Per-Class Error: 0.06666666666666667\n",
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>2</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>9.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0 / 9</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>10.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0 / 10</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td>\n",
       "<td>0.2</td>\n",
       "<td>1 / 5</td></tr>\n",
       "<tr><td>9.0</td>\n",
       "<td>11.0</td>\n",
       "<td>4.0</td>\n",
       "<td>0.0416667</td>\n",
       "<td>1 / 24</td></tr></table></div>"
      ],
      "text/plain": [
       "0    1    2    Error      Rate\n",
       "---  ---  ---  ---------  ------\n",
       "9    0    0    0          0 / 9\n",
       "0    10   0    0          0 / 10\n",
       "0    1    4    0.2        1 / 5\n",
       "9    11   4    0.0416667  1 / 24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Hit Ratios: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>k</b></td>\n",
       "<td><b>hit_ratio</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.9583333</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "k    hit_ratio\n",
       "---  -----------\n",
       "1    0.958333\n",
       "2    1\n",
       "3    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "performance_xgb = xgb_h2o.model_performance(test_data=test)\n",
    "print(performance_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting ist außerdem in [H2O's AutoML Funktion](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) enthalten. AutoML steht für automatisches Machine Learning und vergleicht mit einer Funktion mehrere Algorithmen, wie Gradient Boosting, Random Forests und Neuronale Netze, sowie verschiedene Hyperparameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
