{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# NLP Intent Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hallo und herzlich willkommen zum codecentric.AI bootcamp!\n",
    "\n",
    "Heute wollen wir uns mit einem fortgeschrittenen Thema aus dem Bereich _natural language processing_, kurz _NLP_, genannt, beschäftigen:\n",
    "\n",
    "> Wie bringt man Sprachassistenten, Chatbots und ähnlichen Systemen bei, die Absicht eines Nutzers aus seinen Äußerungen zu erkennen?\n",
    "\n",
    "Dieses Problem wird im Englischen allgemein als _intent recognition_ bezeichnet und gehört zu dem ambitionierten Gebiet des _natural language understanding_, kurz _NLU_ genannt. Einen Einstieg in dieses Thema bietet das folgende [Youtube-Video](https://www.youtube.com/watch?v=H_3R8inCOvM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850\"\n",
       "            height=\"650\"\n",
       "            src=\"https://www.youtube.com/embed/H_3R8inCOvM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff78838dcf8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lade Video\n",
    "from IPython.display import IFrame\n",
    "IFrame('https://www.youtube.com/embed/H_3R8inCOvM', width=850, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Zusammen werden wir in diesem Tutorial mit Hilfe der NLU-Bibliothek [Rasa-NLU](https://rasa.com/docs/nlu/) einem WetterBot beibringen, einfache Fragemuster zum Wetter zu verstehen und zu beantworten. Zum Beispiel wird er auf die Fragen\n",
    "\n",
    "> `\"Wie warm war es 1989?\"`\n",
    "\n",
    "mit\n",
    "\n",
    "<img src=\"img/answer-1.svg\" width=\"85%\" align=\"middle\">\n",
    "\n",
    "und auf\n",
    "\n",
    "> `\"Welche Temperatur hatten wir in Schleswig-Holstein und in Baden-Württemberg?\"`\n",
    "\n",
    "mit\n",
    "\n",
    "<img src=\"img/answer-2.svg\" width=\"85%\" align=\"middle\">\n",
    "\n",
    "antworten. Der folgende Screencast gibt einen Überblick über das Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850\"\n",
       "            height=\"650\"\n",
       "            src=\"https://www.youtube.com/embed/pVwO4Brs4kY\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff78ca924a8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lade Video\n",
    "from IPython.display import IFrame\n",
    "IFrame('https://www.youtube.com/embed/pVwO4Brs4kY', width=850, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Damit es gleich richtig losgehen kann, importieren wir noch zwei Standardbibliotheken und vereinbaren das Datenverzeichnis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Unser Ausgangspunkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Allgemein ist die Aufgabe, aus einer Sprachäußerung die zugrunde liegende Absicht zu erkennen, selbst für Menschen manchmal nicht einfach. Soll ein Computer diese schwierige Aufgabe lösen, so muss man sich überlegen, was man zu einem gegebenen Input &mdash; also einer (unstrukturierten) Sprachäußerung &mdash; für einen Output erwarten, wie man also Absichten modelliert und strukturiert.\n",
    "\n",
    "Weit verbreitet ist folgender Ansatz für Intent Recognition:\n",
    "\n",
    "- jede Äußerung wird einer _Domain_, also einem Gebiet, zugeordnet,\n",
    "- für jede _Domain_ gibt es einen festen Satz von _Intents_, also eine Reihe von Absichten,\n",
    "- jede Absicht kann durch _Parameter_ konkretisiert werden und hat dafür eine Reihe von _Slots_, die wie Parameter einer Funktion oder Felder eines Formulares mit gewissen Werten gefüllt werden können.\n",
    "\n",
    "Für die Äußerungen\n",
    "\n",
    ">  - `\"Wie warm war es 1990 in Berlin?\"`\n",
    ">  - `\"Welche Temperatur hatten wir in Hessen im Jahr 2018?\"`\n",
    ">  - `\"Wie komme ich zum Hauptbahnhof?\"`\n",
    "\n",
    "könnte _Intent Recognition_ also zum Beispiel jeweils folgende Ergebnisse liefern:\n",
    "\n",
    "> - `{'intent': 'Frag_Temperatur', 'slots': {'Ort': 'Berlin', 'Jahr': '1990'}}`\n",
    "> - `{'intent': 'Frag_Temperatur', 'slots': {'Ort': 'Hessen', 'Jahr': '2018'}}`\n",
    "> - `{'intent': 'Frag_Weg', 'slots': {'Start': None, 'Ziel': 'Hauptbahnhof'}}`\n",
    "\n",
    "Für Python steht eine ganze von NLP-Bibliotheken zur Verfügung, die Intent Recognition in der einen oder anderen Form ermöglichen, zum Beispiel\n",
    "\n",
    "- [Rasa NLU](https://rasa.com/docs/nlu/) (&bdquo;Language Understanding for chatbots and AI assistants&ldquo;),\n",
    "- [snips](https://snips-nlu.readthedocs.io/en/latest/) (&bdquo;Using Voice to Make Technology Disappear&ldquo;),\n",
    "- [DeepPavlov](http://deeppavlov.ai) (&bdquo;an open-source conversational AI library&ldquo;),\n",
    "- [NLP Architect](http://nlp_architect.nervanasys.com/index.html) von Intel (&bdquo;for exploring state-of-the-art deep learning topologies and techniques for natural language processing and natural language unterstanding&ldquo;),\n",
    "- [pytext](https://pytext-pytext.readthedocs-hosted.com/en/latest/index.html) von Facebook (&bdquo;a deep-learning based NLP modeling framework built on PyTorch&ldquo;).\n",
    "\n",
    "Wir entscheiden uns im Folgenden für die Bibliothek Rasa NLU, weil wir dafür bequem mit einem Open-Source-Tool (chatette) umfangreiche Trainingsdaten generieren können. Rasa NLU wiederum benutzt  die NLP-Bibliothek [spaCy](https://spacy.io), die Machine-Learning-Bibliothek [scikit-learn](https://scikit-learn.org/stable/) und die Deep-Learning-Bibliothek [TensorFlow](https://www.tensorflow.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Intent Recognition von Anfang bis Ende mit Rasa NLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Schauen wir uns an, wie man eine Sprach-Engine für Intent Recognition trainieren kann! Dafür beschränken wir uns zunächst auf wenige Intents und Trainingsdaten und gehen die benötigten Schritte von Anfang bis Ende durch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Schritt 1: Intents durch Trainingsdaten beschreiben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als Erstes müssen wir die Intents mit Hilfe von Trainingsdaten beschreiben. _Rasa NLU_ erwartet beides zusammen in einer Datei im menschenfreundlichen [Markdown-Format](http://markdown.de/) oder im computerfreundlichen [JSON-Format](https://de.wikipedia.org/wiki/JavaScript_Object_Notation). Ein Beispiel für solche Trainingsdaten im Markdown-Format ist der folgende Python-String, den wir in die Datei `intents.md` speichern: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_INTENTS = \"\"\"\n",
    "## intent: Frag_Temperatur\n",
    "- Wie [warm](Eigenschaft) war es [1900](Zeit) in [Brandenburg](Ort)\n",
    "- Wie [kalt](Eigenschaft) war es in [Hessen](Ort) [1900](Zeit)\n",
    "- Was war die Temperatur [1977](Zeit) in [Sachsen](Ort)\n",
    "\n",
    "## intent: Frag_Ort\n",
    "- Wo war es [1998](Zeit) am [kältesten](Superlativ:kalt)\n",
    "- Finde das [kältesten](Superlativ:kalt) Bundesland im Jahr [2004](Zeit)\n",
    "- Wo war es [2010](Zeit) [kälter](Komparativ:kalt) als [1994](Zeit) in [Rheinland-Pfalz](Ort)\n",
    "\n",
    "## intent: Frag_Zeit\n",
    "- Wann war es in [Bayern](Ort) am [kühlsten](Superlativ:kalt)\n",
    "- Finde das [kälteste](Superlativ:kalt) Jahr im [Saarland](Ort)\n",
    "- Wann war es in [Schleswig-Holstein](Ort) [wärmer](Komparativ:warm) als in [Baden-Württemberg](Ort)\n",
    "\n",
    "## intent: Ende\n",
    "- Ende\n",
    "- Auf Wiedersehen\n",
    "- Tschuess\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "INTENTS_PATH = os.path.join(DATA_DIR, 'intents.md')\n",
    "\n",
    "\n",
    "def write_file(filename, text):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "write_file(INTENTS_PATH, TRAIN_INTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hier wird jeder Intent erst in der Form\n",
    "\n",
    "> `## intent: NAME`\n",
    "\n",
    "deklariert, wobei `NAME` durch die Bezeichnung des Intents zu ersetzen ist. Anschließend wird der Intent durch eine Liste von\n",
    "Beispiel-Äußerungen beschrieben. Die Parameter beziehungsweise Slots werden in den Beispieläußerungen in der Form\n",
    "\n",
    "> `[WERT](SLOT)`\n",
    "\n",
    "markiert, wobei `SLOT` die Bezeichnung des Slots und `Wert` der entsprechende Teil der Äußerung ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Schritt 2: Sprach-Engine konfigurieren..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Sprach-Engine von _Rasa NLU_ ist als Pipeline gestaltet und [sehr flexibel konfigurierbar](https://rasa.com/docs/nlu/components/#section-pipeline). Zwei [Beispiel-Konfigurationen](https://rasa.com/docs/nlu/choosing_pipeline/) sind in Rasa bereits enthalten:\n",
    "\n",
    "- `spacy_sklearn` verwendet vortrainierte Wortvektoren, eine [scikit-learn-Implementierung](https://scikit-learn.org/stable/modules/svm.html) einer linearen [Support-vector Machine]( https://en.wikipedia.org/wiki/Support-vector_machine) für die Klassifikation und wird für kleine Trainingsmengen (<1000) empfohlen. Da diese Pipeline vortrainierte Wortvektoren und spaCy benötigt, kann sie nur für [die meisten westeuropäische Sprachen](https://rasa.com/docs/nlu/languages/#section-languages) verwendet werden. Allerdings sind die Version 0.20.1 von scikit-learn und 0.13.8 von Rasa-NLU nicht kompatibel\n",
    "\n",
    "- `tensorflow_embedding` trainiert für die Klassifikation Einbettungen von Äußerungen und von Intents in denselben Vektorraum  und wird für größere Trainingsmengen (>1000) empfohlen. Die zu Grunde liegende Idee stammt aus dem Artikel [StarSpace: Embed All The Things!](https://arxiv.org/abs/1709.03856). Sie ist sehr vielseitig anwendbar und beispielsweise auch für  [Question Answering](https://en.wikipedia.org/wiki/Question_answering) geeignet. Diese Pipeline benötigt kein Vorwissen über die verwendete Sprache, ist also universell einsetzbar, und kann auch auf das Erkennen mehrerer Intents in einer Äußerung trainiert werden.\n",
    "\n",
    "Zum Füllen der Slots verwenden beide Pipelines eine [Python-Implementierung](http://www.chokkan.org/software/crfsuite/) von [Conditional Random Fields](https://en.wikipedia.org/wiki/Conditional_random_field).\n",
    "\n",
    "Die Konfiguration der Pipeline wird durch eine YAML-Datei beschrieben. Der folgende Python-String entspricht der Konfiguration `tensorflow_embedding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG_TF = \"\"\"\n",
    "pipeline:\n",
    "- name: \"tokenizer_whitespace\"\n",
    "- name: \"ner_crf\"\n",
    "- name: \"ner_synonyms\"\n",
    "- name: \"intent_featurizer_count_vectors\"\n",
    "- name: \"intent_classifier_tensorflow_embedding\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Schritt 3: ...trainieren..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Sind die Trainingsdaten und die Konfiguration der Pipeline beisammen, so kann die Sprach-Engine trainiert werden. In der Regel erfolgt dies bei Rasa mit Hilfe eines Kommandozeilen-Interface oder direkt [in Python](https://rasa.com/docs/nlu/python/). Die folgende Funktion `train` erwartet die Konfiguration als Python-String und den Namen der Datei mit den Trainingsdaten und gibt die trainierte Sprach-Engine als Instanz einer `Interpreter`-Klasse zurück:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/rasa_nlu/utils/__init__.py:239: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return yaml.load(read_file(filename, \"utf-8\"))\n",
      "Epochs: 100%|██████████| 300/300 [00:02<00:00, 146.06it/s, loss=0.097, acc=1.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /notebooks/nlp_intent/models/default/model_20191006-125739/intent_classifier_tensorflow_embedding.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import rasa_nlu.training_data\n",
    "import rasa_nlu.config\n",
    "from rasa_nlu.model import Trainer, Interpreter\n",
    "\n",
    "MODEL_DIR = 'models'\n",
    "\n",
    "def train(config=CONFIG_TF, intents_path=INTENTS_PATH):\n",
    "    config_path = os.path.join(DATA_DIR, 'rasa_config.yml')\n",
    "    write_file(config_path, config)\n",
    "    trainer = Trainer(rasa_nlu.config.load(config_path))\n",
    "    trainer.train(rasa_nlu.training_data.load_data(intents_path))\n",
    "    return Interpreter.load(trainer.persist(MODEL_DIR))\n",
    "\n",
    "interpreter = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Schritt 4: ...und testen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir testen nun, ob die Sprach-Engine `interpreter` folgende Test-Äußerungen richtig versteht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TEST_UTTERANCES = [\n",
    "    'Was war die durchschnittliche Temperatur 2004 in Mecklenburg-Vorpommern',\n",
    "    'Nenn mir das wärmste Bundesland 2018',\n",
    "    'In welchem Jahr war es in Nordrhein-Westfalen heißer als 1990',\n",
    "    'Wo war es 2000 am kältesten',\n",
    "    'Bis bald',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Methode `parse` von `interpreter` erwartet eine Äußerung als Python-String, wendet Intent Recognition an und liefert eine sehr detaillierte Rückgabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'Frag_Temperatur', 'confidence': 0.9676849842071533},\n",
       " 'entities': [{'start': 41,\n",
       "   'end': 45,\n",
       "   'value': '2004',\n",
       "   'entity': 'Zeit',\n",
       "   'confidence': 0.8843208566489816,\n",
       "   'extractor': 'ner_crf'},\n",
       "  {'start': 49,\n",
       "   'end': 71,\n",
       "   'value': 'Mecklenburg-Vorpommern',\n",
       "   'entity': 'Ort',\n",
       "   'confidence': 0.9251510709571028,\n",
       "   'extractor': 'ner_crf'}],\n",
       " 'intent_ranking': [{'name': 'Frag_Temperatur',\n",
       "   'confidence': 0.9676849842071533},\n",
       "  {'name': 'Ende', 'confidence': 0.0},\n",
       "  {'name': 'Frag_Zeit', 'confidence': 0.0},\n",
       "  {'name': 'Frag_Ort', 'confidence': 0.0}],\n",
       " 'text': 'Was war die durchschnittliche Temperatur 2004 in Mecklenburg-Vorpommern'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.parse(TEST_UTTERANCES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Rückgabe umfasst im Wesentlichen\n",
    "\n",
    "- den Namen des ermittelten Intent sowie eine Sicherheit beziehungsweise Konfidenz zwischen 0 und 1,\n",
    "- für jeden ermittelten Parameter die Start- und Endposition in der Äußerung, den Wert und wieder eine Konfidenz,\n",
    "- ein Ranking der möglichen Intents nach der Sicherheit/Konfidenz, mit der sie in dieser Äußerung vermutet wurden.\n",
    "\n",
    "Für eine übersichtlichere Darstellung und leichte Weiterverarbeitung bereiten wir die Rückgabe mit Hilfe der Funktionen `extract_intent` und `extract_confidences` ein wenig auf. Anschließend gehen wir unsere Test-Äußerungen durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Was war die durchschnittliche Temperatur 2004 in Mecklenburg-Vorpommern\n",
      "> ('Frag_Temperatur', [('Zeit', '2004'), ('Ort', 'Mecklenburg-Vorpommern')])\n",
      "  (0.9676849842071533, [0.8843208566489816, 0.9251510709571028])\n",
      "\n",
      "< Nenn mir das wärmste Bundesland 2018\n",
      "> ('Frag_Ort', [('Zeit', '2018')])\n",
      "  (0.9345992803573608, [0.8188949907951559])\n",
      "\n",
      "< In welchem Jahr war es in Nordrhein-Westfalen heißer als 1990\n",
      "> ('Frag_Zeit', [('Ort', 'Nordrhein-Westfalen'), ('Komparativ', 'heißer'), ('Zeit', '1990')])\n",
      "  (0.5457459688186646, [0.8231770104558299, 0.5422260768239187, 0.8920504023927753])\n",
      "\n",
      "< Wo war es 2000 am kältesten\n",
      "> ('Frag_Ort', [('Zeit', '2000'), ('Superlativ', 'kalt')])\n",
      "  (0.9631474018096924, [0.8707385652923301, 0.9376147531759642])\n",
      "\n",
      "< Bis bald\n",
      "> (None, [])\n",
      "  (0.0, [])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_intent(intent):\n",
    "    return (intent['intent']['name'] if intent['intent'] else None,\n",
    "            [(ent['entity'], ent['value']) for ent in intent['entities']])\n",
    "\n",
    "\n",
    "def extract_confidences(intent):\n",
    "    return (intent['intent']['confidence'] if intent['intent'] else None,\n",
    "           [ent['confidence'] for ent in intent['entities']])\n",
    "\n",
    "\n",
    "def test(interpreter, utterances=TEST_UTTERANCES):\n",
    "    for utterance in utterances:\n",
    "        intent = interpreter.parse(utterance)\n",
    "        print('<', utterance)\n",
    "        print('>', extract_intent(intent))\n",
    "        print(' ', extract_confidences(intent))\n",
    "        print()\n",
    "\n",
    "test(interpreter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das Ergebnis ist noch nicht ganz überzeugend &mdash;  wir haben aber auch nur ganz wenig Trainingsdaten vorgegeben!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##  Trainingsdaten generieren mit Chatette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Für ein erfolgreiches Training brauchen wir also viel mehr Trainingsdaten. Doch fängt man an, weitere Beispiele aufzuschreiben, so fallen einem schnell viele kleine Variationsmöglichkeiten ein, die sich recht frei kombinieren lassen. Zum Beispiel können wir für eine Frage nach der Temperatur in Berlin im Jahr 1990 mit jeder der Phrasen\n",
    "> - \"Wie warm war es...\"\n",
    "> - \"Wie kalt war es...\"\n",
    "> - \"Welche Temperatur hatten wir...\"\n",
    "\n",
    "beginnen und dann mit\n",
    "\n",
    "> - \"...in Berlin 1990\"\n",
    "> - \"...1990 in Berlin\"\n",
    "\n",
    "abschließen, vor \"1990\" noch \"im Jahr\" einfügen und so weiter. Statt alle denkbaren Kombinationen aufzuschreiben, ist es sinnvoller, die Möglichkeiten mit Hilfe von Regeln zu beschreiben und daraus Trainingsdaten generieren zu lassen. Genau das ermöglicht das Python-Tool [chatette](https://github.com/SimGus/Chatette), das wir im Folgenden verwenden. Dieses Tool liest Regeln, die einer speziellen Syntax folgen müssen, aus einer Datei aus und erzeugt dann daraus Trainingsdaten für Rasa NLU im JSON-Format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Regeln zur Erzeugung von Trainingsdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir legen im Folgenden erst einen Grundvorrat an Regeln für die Intents `Frag_Temperatur`, `Frag_Ort`, `Frag_Zeit` und `Ende` in einem Python-Dictionary an und erläutern danach genauer, wie die Regeln aufgebaut sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "RULES = {\n",
    "    '@[Ort]': (\n",
    "        'Brandenburg', 'Baden-Wuerttemberg', 'Bayern', 'Hessen',\n",
    "        'Rheinland-Pfalz', 'Schleswig-Holstein', 'Saarland', 'Sachsen',\n",
    "    ),\n",
    "    '@[Zeit]': set(map(str, np.random.randint(1891, 2018, size=5))),\n",
    "    '@[Komparativ]': ('wärmer', 'kälter',),\n",
    "    '@[Superlativ]': ('wärmsten', 'kältesten',),\n",
    "    '%[Frag_Temperatur]': ('Wie {warm/kalt} war es ~[zeit_ort]',\n",
    "                  'Welche Temperatur hatten wir ~[zeit_ort]',\n",
    "                  'Wie war die Temperatur ~[zeit_ort]',\n",
    "    ),\n",
    "    '%[Frag_Ort]': (\n",
    "        '~[wo_war] es @[Zeit] @[Komparativ] als {@[Zeit]/in @[Ort]}',\n",
    "        '~[wo_war] es @[Zeit] am @[Superlativ]',\n",
    "    ),\n",
    "    '%[Frag_Jahr]': (\n",
    "        '~[wann_war] es in @[Ort] @[Komparativ] als {@[Zeit]/in @[Ort]}',\n",
    "        '~[wann_war] es in @[Ort] am @[Superlativ]',\n",
    "    ),\n",
    "    '%[Ende]': ('Ende', 'Auf Wiedersehen', 'Tschuess',),\n",
    "    '~[finde]': ('Sag mir', 'Finde'),\n",
    "    '~[wie_war]': ('Wie war', '~[finde]',),\n",
    "    '~[was_war]': ('Was war', '~[finde]',),\n",
    "    '~[wo_war]': ('Wo war', 'In welchem {Bundesland|Land} war',),\n",
    "    '~[wann_war]': ('Wann war', 'In welchem Jahr war',),\n",
    "    '~[zeit_ort]': ('@[Zeit] in @[Ort]', '@[Ort] in @[Zeit]',),\n",
    "    '~[Bundesland]': ('Land', 'Bundesland',),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Jede Regel besteht aus einem Namen beziehungsweise Platzhalter und einer Menge von Phrasen. Je nachdem, ob der Name die Form\n",
    "> `%[NAME]`, `@[NAME]` oder `~[NAME]`\n",
    "\n",
    "hat, beschreibt die Regel einen\n",
    "\n",
    "> _Intent_, _Slot_ oder eine _Alternative_\n",
    "\n",
    "mit der Bezeichnung `NAME`.  Jede Phrase kann ihrerseits Platzhalter für Slots und Alternativen erhalten. Diese Platzhalter werden bei der Erzeugung von Trainingsdaten von chatette jeweils durch eine der Phrasen ersetzt, die in der Regel für den jeweiligen Slot beziehungsweise die Alternativen aufgelistet sind. Außerdem können Phrasen\n",
    "\n",
    "- Alternativen der Form `{_|_|_}`,\n",
    "- optionale Teile in der Form `[_?]`\n",
    "\n",
    "und einige weitere spezielle Konstrukte enthalten. Mehr Details finden sich in der [Syntax-Beschreibung](https://github.com/SimGus/Chatette/wiki/Syntax-specifications) von chatette.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Erzeugung der Trainingsdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die in dem Python-Dictionary kompakt abgelegten Regeln müssen nun für chatette so formatiert werden, dass bei jeder Regel der Name einen neuen Absatz einleitet und anschließend die möglichen Phrasen schön eingerückt Zeile für Zeile aufgelistet werden. Dies leistet die folgende Funktion `format_rules`.  Zusätzlich fügt sie eine Vorgabe ein, wieviel Trainingsbeispiele pro Intent erzeugt werden sollen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def format_rules(rules, train_samples):\n",
    "    train_str =  \"('training':'{}')\".format(train_samples)\n",
    "    llines = [[name if (name[0] != '%') else name + train_str]\n",
    "              + ['    ' + val for val in rules[name]] + [''] for name in rules]\n",
    "    return '\\n'.join((l for lines in llines for l in lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun wenden wir chatette an, um die Trainingsdaten zu generieren. Dafür bietet chatette ein bequemes [Kommandozeilen-Interface](https://github.com/SimGus/Chatette/wiki/Command-line-interface), aber wir verwenden direkt die zu Grunde liegenden Python-Module.\n",
    "\n",
    "Die folgende Funktion `chatette` erwartet wie `format_rules` ein Python-Dictionary mit Regeln, schreibt diese passend formatiert in eine Datei, löscht etwaige zuvor generierte Trainingsdateien und erzeugt dann den Regeln entsprechend neue Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG] Parsing file: data/intents.chatette\n",
      "[DBG] Parsing of file: data/intents.chatette finished\n",
      "[DBG] Generating training examples...\n"
     ]
    }
   ],
   "source": [
    "from chatette.adapters import RasaAdapter\n",
    "from chatette.parsing import Parser\n",
    "from chatette.generator import Generator\n",
    "import glob\n",
    "\n",
    "TRAIN_SAMPLES = 400\n",
    "CHATETTE_DIR = os.path.join(DATA_DIR, 'chatette')\n",
    "\n",
    "\n",
    "def chatette(rules=RULES, train_samples=TRAIN_SAMPLES):\n",
    "    rules_path = os.path.join(DATA_DIR, 'intents.chatette')\n",
    "    write_file(rules_path, format_rules(rules, train_samples))\n",
    "    with open(rules_path, 'r') as rule_file:\n",
    "        parser = Parser(rule_file)\n",
    "        parser.parse()\n",
    "    generator = Generator(parser)\n",
    "    for f in glob.glob(os.path.join(CHATETTE_DIR, '*')):\n",
    "        os.remove(f)\n",
    "    RasaAdapter().write(CHATETTE_DIR, list(generator.generate_train()),\n",
    "                        generator.get_entities_synonyms())\n",
    "    \n",
    "chatette(train_samples=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Und nun: neuer Test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Bringen die umfangreicheren Trainingsdaten wirklich eine Verbesserung? Schauen wir's uns an! Um verschiedene Sprach-Engines zu vergleichen, nutzen wir die folgende Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/rasa_nlu/utils/__init__.py:239: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return yaml.load(read_file(filename, \"utf-8\"))\n",
      "Epochs: 100%|██████████| 300/300 [00:21<00:00, 13.93it/s, loss=0.003, acc=1.000]\n",
      "/usr/local/lib/python3.6/site-packages/rasa_nlu/extractors/entity_synonyms.py:85: UserWarning: Failed to load synonyms file from '/notebooks/nlp_intent/models/default/model_20191006-125804/entity_synonyms.json'\n",
      "  \"\".format(entity_synonyms_file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /notebooks/nlp_intent/models/default/model_20191006-125804/intent_classifier_tensorflow_embedding.ckpt\n",
      "< Was war die durchschnittliche Temperatur 2004 in Mecklenburg-Vorpommern\n",
      "> ('Frag_Temperatur', [('Zeit', '2004'), ('Ort', 'Mecklenburg-Vorpommern')])\n",
      "  (0.9136318564414978, [0.8541794001234732, 0.9875637470644905])\n",
      "\n",
      "< Nenn mir das wärmste Bundesland 2018\n",
      "> ('Frag_Ort', [('Zeit', '2018')])\n",
      "  (0.6670867204666138, [0.9205167054194056])\n",
      "\n",
      "< In welchem Jahr war es in Nordrhein-Westfalen heißer als 1990\n",
      "> ('Frag_Jahr', [('Ort', 'Nordrhein-Westfalen'), ('Komparativ', 'heißer'), ('Zeit', '1990')])\n",
      "  (0.9601661562919617, [0.968290681457331, 0.9009353187976876, 0.9976314157284462])\n",
      "\n",
      "< Wo war es 2000 am kältesten\n",
      "> ('Frag_Ort', [('Zeit', '2000'), ('Superlativ', 'kältesten')])\n",
      "  (0.9656134247779846, [0.889369756926878, 0.9902646526673686])\n",
      "\n",
      "< Bis bald\n",
      "> (None, [])\n",
      "  (0.0, [])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_test(config=CONFIG_TF, utterances=TEST_UTTERANCES):\n",
    "    interpreter = train(config, CHATETTE_DIR)\n",
    "    test(interpreter, utterances)\n",
    "    return interpreter\n",
    "\n",
    "interpreter = train_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hier wurde nur die letzte Äußerung nicht verstanden, aber das ist auch nicht weiter verwunderlich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##  Unser kleiner WetterBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Experimentieren macht mehr Spaß, wenn es auch mal zischt und knallt. Oder zumindest irgendeine andere Reaktion erfolgt. Und deswegen bauen wir uns einen kleinen WetterBot, der auf die erkannten Intents reagieren kann. Zuerst schreiben wir dafür eine Eingabe-Verarbeitungs-Ausgabe-Schleife. Diese erwartet als Parameter erstens die Sprach-Engine `interpreter` und zweitens ein Python-Dictionary `handlers`, welches jeder Intent-Bezeichnung einen Handler zuordnet. Der Handler wird dann mit dem erkannten Intent aufgerufen und sollte zurückgeben, ob die Schleife fortgeführt werden soll oder nicht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def dialog(interpreter, handlers):\n",
    "    quit = False\n",
    "    while not quit:\n",
    "        intent = extract_intent(interpreter.parse(input('>')))\n",
    "        print('<', intent)\n",
    "        intent_name = intent[0]\n",
    "        if intent_name in handlers:\n",
    "            quit = handlers[intent_name](intent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir implementieren gleich beispielhaft einen Handler für den Intent `Frag_Temperatur`und reagieren auf alle anderen Intents mit einer Standard-Antwort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def message(msg, quit=False):\n",
    "    print(msg)\n",
    "    return quit\n",
    "\n",
    "HANDLERS = { \n",
    "    'Ende': lambda intent: message('=> Oh, wie schade. Bis bald!', True),\n",
    "    'Frag_Zeit': lambda intent: message('=> Das ist eine gute Frage.'),\n",
    "    'Frag_Ort': lambda intent: message('=> Dafür wurde ich nicht programmiert.'),\n",
    "    'Frag_Temperatur': lambda intent: message('=> Das weiss ich nicht.')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Um die Fragen nach den Temperaturen zu beantworten, nutzen wir [Archiv-Daten](ftp://ftp-cdc.dwd.de/pub/CDC/regional_averages_DE/annual/air_temperature_mean/regional_averages_tm_year.txt) des [Deutschen Wetterdienstes](https://www.dwd.de), die wir schon etwas aufbereitet haben. Die Routine `show` gibt die nachgefragten Temperaturdaten je nach Anzahl der angegebenen Jahre und Bundesländer als Liniendiagramm, Balkendiagramm oder in Textform an. Der eigentliche Hander `frag_wert` prüft, ob die angegebenen Jahre und Orte auch zulässig sind und setzt, falls eine der beiden Angaben fehlt, einfach alle Jahre beziehungsweise Bundesländer ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "sns.set()\n",
    "\n",
    "DATA_PATH = os.path.join(DATA_DIR, 'temperaturen.txt')\n",
    "temperature = pd.read_csv(DATA_PATH, index_col=0, sep=';')\n",
    "\n",
    "def show(times, places):\n",
    "    if (len(places) == 0) and (len(times) == 0):\n",
    "        print('Keine zulässigen Orte oder Zeiten')\n",
    "    elif (len(places) == 1) and (len(times) == 1):\n",
    "        print(temperature.loc[times, places])\n",
    "    else:\n",
    "        if (len(places) > 1) and (len(times) == 1):\n",
    "            temperature.loc[times[0], places].plot.barh()\n",
    "        if (len(places) == 1) and (len(times) > 1):\n",
    "            temperature.loc[times, places[0]].plot.line()\n",
    "        if (len(places) > 1) and (len(times) > 1):\n",
    "            temperature.loc[times, places].plot.line()\n",
    "            plt.legend(bbox_to_anchor=(1.05,1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "\n",
    "def frag_temperatur(intent):\n",
    "    def validate(options, ent_name, fn):\n",
    "        chosen = [fn(value) for (name, value) in intent[1] if name == ent_name]\n",
    "        return list(set(options) & set(chosen)) if chosen else options\n",
    "    places = validate(list(temperature.columns), 'Ort', lambda x:x)\n",
    "    times = validate(list(temperature.index), 'Zeit', int)\n",
    "    show(times, places)\n",
    "    return False\n",
    "\n",
    "HANDLERS['Frag_Temperatur'] = frag_temperatur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun kann der WetterBot getestet werden! Zum Beispiel mit\n",
    "\n",
    ">  \"Wie warm war es in Baden-Württemberg und Sachsen?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< (None, [])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< ('Ende', [])\n",
      "=> Oh, wie schade. Bis bald!\n"
     ]
    }
   ],
   "source": [
    "dialog(interpreter, HANDLERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Intent Recognition selbst gemacht &mdash; ein Bi-LSTM-Netzwerk mit Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Im Prinzip haben wir nun gesehen, wie sich Intent Recognition mit Hilfe von Rasa NLU recht einfach anwenden lässt. Aber wie funktioniert das ganz genau? In diesem zweiten Teil des Notebooks werden wir\n",
    "\n",
    "  - ein bidirektionales rekurrentes Netz, wie es im Video vorgestellt wurde, implementieren,\n",
    "  - die mit chatette erstellten Trainingsdaten so aufbereiten, dass wir damit das Netz trainieren können,\n",
    "\n",
    "und sehen, dass das ganz gut klappt und gar nicht so schwer ist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Intents einlesen und aufbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Zuerst lesen wir die Trainings-Daten, die von chatette im JSON-Format ausgegeben in die Date `RASA_INTENTS` geschrieben wurden, aus, und schauen uns das Format der Einträge an:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "CHATETTE_DIR = os.path.join(DATA_DIR, 'chatette')\n",
    "RASA_INTENTS = os.path.join(CHATETTE_DIR, 'output.json')\n",
    "\n",
    "\n",
    "def load_intents():\n",
    "    with open(RASA_INTENTS) as intents_file:\n",
    "        intents = json.load(intents_file)\n",
    "    return intents['rasa_nlu_data']['common_examples']\n",
    "\n",
    "\n",
    "sample_intent = load_intents()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wie bereits im [Video](https://www.youtube.com/watch?v=H_3R8inCOvM) erklärt, sind für Intent Recognition zwei Aufgaben zu lösen:\n",
    "\n",
    "  - die _Klassifikation_ des Intent anhand der gegebenen Äußerung und\n",
    "  - das Füllen der Slots.\n",
    "\n",
    "Die zweite Aufgabe kann man als _Sequence Tagging_ auffassen &mdash; für jedes Token der Äußerung ist zu bestimmen, ob es den Parameter für einen Slot darstellt oder nicht. Für den Beispiel-Intent\n",
    "\n",
    ">   `{'entities': [{'end': 20, 'entity': 'Zeit', 'start': 16, 'value': '1993'},\n",
    ">                  {'end': 35, 'entity': 'Ort', 'start': 24, 'value': 'Brandenburg'}],\n",
    ">     'intent': 'Frag_Temperatur',\n",
    ">     'text': 'Wie warm war es 1993 in Brandenburg'}`\n",
    "\n",
    "wäre die Eingabe für diese beiden Aufgaben also die Token-Folge\n",
    "\n",
    "> `['Wie', 'warm', 'war', 'es', '1993', 'in', 'Brandenburg']`\n",
    "\n",
    "und die gewünschte Ausgabe jeweils\n",
    "\n",
    "> `'Frag_Temperatur'`\n",
    "\n",
    "beziehungsweise die Tag-Folge\n",
    "\n",
    "> `['-', '-', '-', '-', 'Zeit', '-', 'Ort']`\n",
    "\n",
    "Die folgende Funktion extrahiert aus den geladenen Beispiel-Intents die gewünschte Eingabe und die Ausgaben für diese beiden Aufgaben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Frag_Temperatur',\n",
       " [Wie, warm, war, es, 1977, in, Brandenburg],\n",
       " ['-', '-', '-', '-', 'Zeit', '-', 'Ort'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from itertools import accumulate\n",
    "\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [word for word in nlp(text)]\n",
    "\n",
    "NO_ENTITY = '-'\n",
    "\n",
    "def intent_and_sequences(intent):\n",
    "    def get_tag(offset):\n",
    "        \"\"\"Returns the tag (+slot name) for token starting at `offset`\"\"\"\n",
    "        ents = [ent['entity'] for ent in intent['entities'] if ent['start'] == offset]\n",
    "        return ents[0] if ents else NO_ENTITY\n",
    "    token = tokenize(intent['text'])\n",
    "    # `offsets` is the list of starting positions of the token\n",
    "    offsets = list(accumulate([0,] + [len(t.text_with_ws) for t in token]))\n",
    "    return (intent['intent'], token, list(map(get_tag, offsets[:-1])))\n",
    "\n",
    "intent_and_sequences(sample_intent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Symbolische Daten in numerische Daten umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die aufbereiteten Intents enthalten nun jeweils\n",
    "\n",
    "  1. die Folge der Token als \"Eingabe\"\n",
    "  2. den Namen des Intent als Ergebnis der Klassifikation und\n",
    "  3. die Folge der Slot-Namen als Ergebnis des Sequence Tagging.\n",
    "\n",
    "Diese kategoriellen Daten müssen wir für die Weiterverarbeitung in numerische Daten umwandeln. Dafür bieten sich\n",
    "\n",
    "  - für 1. Wortvektoren und\n",
    "  - für 2. und 3. die One-hot-Kodierung an.\n",
    "\n",
    "Außerdem müssen wir die Eingabe-Folge und Tag-Folge auf eine feste Länge bringen.\n",
    "\n",
    "Beginnen wir mit der One-hot-Kodierung. Die folgende Funktion erzeug zu einer gegebenen Menge von Objekten ein Paar von Python-Dictionaries, welche jedem Objekt einen One-hot-Code und umgekehrt jedem Index das entsprechende Objekt zuordnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def ohe(s):\n",
    "    codes = np.eye(len(s))\n",
    "    numerated = list(enumerate(s))\n",
    "    return ({value: codes[idx] for (idx, value) in numerated},\n",
    "            {idx: value for (idx, value) in numerated})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die nächste Hilfsfunktion erwartet eine Liste von Elementen und schneidet diese auf eine vorgegebene Länge beziehungsweise füllt sie mit einem vorgegebenen Element auf diese Länge auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def fill(items, max_len, filler):\n",
    "    if len(items) < max_len:\n",
    "        return items + [filler] * (max_len - len(items))\n",
    "    else:\n",
    "        return items[0:max_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Umwandlung der aufbereiteten Intent-Tripel in numerische Daten verpacken wir in einen [scikit-learn-Transformer](https://scikit-learn.org/stable/data_transforms.html), weil während der Umwandlung die One-Hot-Kodierung der Intent-Namen und Slot-Namen gelernt und eventuell später für neue Testdaten wieder gebraucht wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "MAX_LEN = 20\n",
    "VEC_DIM = len(list(nlp(' '))[0].vector)\n",
    "\n",
    "class IntentNumerizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        intent_names = set((x[0] for x in X))\n",
    "        self.intents_ohe, self.idx_intents = ohe(intent_names)\n",
    "        self.nr_intents = len(intent_names)\n",
    "        tag_lists = list(map(lambda x: set(x[2]), X)) + [[NO_ENTITY]]\n",
    "        tag_names = frozenset().union(*tag_lists)\n",
    "        # tag_names = set(())\n",
    "        self.tags_ohe, self.idx_tags = ohe(tag_names)\n",
    "        self.nr_tags = len(tag_names)\n",
    "        return self\n",
    "\n",
    "    def transform_utterance(self, token):\n",
    "        return np.stack(fill([tok.vector for tok in token], MAX_LEN,\n",
    "                             np.zeros((VEC_DIM))))\n",
    "\n",
    "    def transform_tags(self, tags):\n",
    "        return np.stack([self.tags_ohe[t] for t in fill(tags, MAX_LEN, NO_ENTITY)]) \n",
    "\n",
    "    def transform(self, X):\n",
    "        return (np.stack([self.transform_utterance(x[1]) for x in X]),\n",
    "                np.stack([self.intents_ohe[x[0]] for x in X]),\n",
    "                np.stack([self.transform_tags(x[2]) for x in X]))\n",
    "\n",
    "    def revert(self, intent_idx, tag_idxs):\n",
    "        return (self.idx_intents[intent_idx],\n",
    "                [self.idx_tags[t] for t in tag_idxs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Keras-Implementierung eines Bi-LSTM-Netzes für Intent Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir implementieren nun mit Keras eine Netz-Architektur, die in [diesem Artikel]() vorgeschlagen wurde und schematisch in folgendem Diagramm dargestellt ist:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"img/birnn.svg\"  style=\"background:white\" width=\"80%\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hierbei wird\n",
    "\n",
    "1. die Eingabe, wie bereits erklärt, als Folge von Wortvektoren dargestellt,\n",
    "2. diese Eingabe erst durch eine rekurrente Schicht forwärts abgearbeitet,\n",
    "3. der Endzustand dieser Schicht als Initialisierung einer sich anschließenden rekurrenten Schicht verwendet, welche die Eingabefolge rückwärts abarbeitet,\n",
    "4. der Endzustand dieser Schicht an eine Schicht mit genau so vielen Neuronen, wie es Intent-Klassen gibt, zur Klassifikation des Intent weitergleitet,\n",
    "5. die Ausgabe der beiden rekurrenten Schichten für jeden Schritt zusammengefügt und\n",
    "6. die zusammengefügte Ausgabe jeweils an ein Bündel von so vielen Neuronen, wie es Slot-Arten gibt, zur Klassifikation des Tags des jeweiligen Wortes weitergeleitet.\n",
    "\n",
    "Genau diesen Aufbau bilden wir nun mit Keras ab, wobei wir die [funktionale API]() benutzen. Als Loss-Funktion verwenden wir jeweils [kategorielle Kreuzentropie](). Für die rekurrenten Schichten verwenden wir [LSTM-Zellen](), auf die wir gleich noch eingehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Concatenate, TimeDistributed, Dense\n",
    "\n",
    "UNITS = 256\n",
    "\n",
    "def build_bilstm(input_dim, nr_intents, nr_tags, units=UNITS):\n",
    "    inputs = Input(shape=(MAX_LEN, input_dim))\n",
    "    lstm_params = {'units': units, 'return_sequences': True, 'return_state': True}\n",
    "    fwd = LSTM(**lstm_params)(inputs)\n",
    "    bwd = LSTM(**lstm_params)(inputs, initial_state=fwd[1:])\n",
    "    merged = Concatenate()([fwd[0], bwd[0]])\n",
    "    tags = TimeDistributed(Dense(nr_tags, activation='softmax'))(merged)\n",
    "    intent = Dense(nr_intents, activation='softmax')(bwd[2])\n",
    "    model = Model(inputs=inputs, outputs=[intent, tags])\n",
    "    model.compile(optimizer='Adam' ,loss='categorical_crossentropy')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Schauen wir uns einmal genauer an, wie so eine LSTM-Zelle aufgebaut ist:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"img/lstm.svg\" style=\"background:white\" width=\"70%\" align=\"middle\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Bezeichnung 'LSTM' steht für _long short-term memory_ und rührt daher, dass solch eine Zelle neben der Eingabe des aktuellen Schrittes nicht nur die Ausgabe des vorherigen Schrittes, sondern zusätzlich auch einen Speicherwert des vorherigen Schrittes erhält. Nacheinander wird in der LSTM-Zelle dann jeweils anhand der aktuellen Eingabe und der vorherigen Ausgabe\n",
    "\n",
    "1. in einem _forget gate_ entschieden, wieviel vom alten Speicherwert vergessen werden soll,\n",
    "2. in einem _input gate_ entschieden, wieviel von der neuen Eingabe in den neuen Speicherwert aufgenommen werden soll,\n",
    "3. in einem _output gate_ aus dem aktuellen Speicher die aktuelle Ausgabe gebildet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Training und Test des Bi-LSTM-Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Schauen wir uns nun an, wie gut das funktioniert! Dazu müssen wir nun alles zusammenfügen und tun das in zwei Schritten.\n",
    "\n",
    "Die Funktion `train_test_data` erwartet als Eingabe Regeln, wie wir sie für chatette in einem Python-Dictionary gespeichert hatten, und liefert die entsprechend erzeugten Intents in numerisch aufbereiter Form, aufgeteilt in Trainings- und Validierungsdaten, einschließlich des angepassten `IntentNumerizer`zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.7\n",
    "\n",
    "def train_test_data(rules=RULES, train_ratio=TRAIN_RATIO):\n",
    "    structured_intents = list(map(intent_and_sequences, load_intents()))\n",
    "    intent_numerizer = IntentNumerizer()\n",
    "    X, y, Y = intent_numerizer.fit_transform(structured_intents)\n",
    "    nr_samples = len(y)\n",
    "    shuffled_indices = np.random.permutation(nr_samples)\n",
    "    split = int(nr_samples * train_ratio)\n",
    "    train_indices, test_indices = (shuffled_indices[0:split], shuffled_indices[split:])\n",
    "    y_train, X_train, Y_train = y[train_indices], X[train_indices], Y[train_indices]\n",
    "    y_test, X_test, Y_test = y[test_indices], X[test_indices], Y[test_indices]\n",
    "    return intent_numerizer, X_train, y_train, Y_train, X_test, y_test, Y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Mit diesen Trainings- und Testdaten trainiert beziehungsweise validiert die folgende Funktion `build_interpreter` nun das von `build_lstm` gebaute neuronale Netz und liefert einen Interpreter-Funktion zurück. Diese erwartet als Eingabe eine Äußerung, transformiert diese anschließend mit dem angepassten  `IntentNumerizer`   und führt mit dem zuvor trainierten Netz die Intent Recognition durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "def build_interpreter(rules=RULES, units=UNITS, batch_size=128, epochs=EPOCHS):\n",
    "    def interpreter(utterance):\n",
    "        x = intent_numerizer.transform_utterance(tokenize(utterance))\n",
    "        y, Y = model.predict(np.stack([x]))\n",
    "        tag_idxs = np.argmax(Y[0], axis=1)\n",
    "        intent_idx = np.argmax(y[0])\n",
    "        return intent_numerizer.revert(intent_idx, tag_idxs)\n",
    "\n",
    "    intent_numerizer, X_train, y_train, Y_train, X_test, y_test, Y_test = train_test_data(rules)\n",
    "    model = build_bilstm(X_train.shape[2], y_train.shape[1], Y_train.shape[2], units)\n",
    "    model.fit(x=X_train, y=[y_train, Y_train],\n",
    "              validation_data=(X_test,[y_test, Y_test]),\n",
    "              batch_size=batch_size, epochs=epochs)\n",
    "    return interpreter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Und nun sind wir bereit zum Testen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 702 samples, validate on 301 samples\n",
      "Epoch 1/10\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 1.7896 - dense_2_loss: 1.0908 - time_distributed_1_loss: 0.6988 - val_loss: 0.5782 - val_dense_2_loss: 0.5287 - val_time_distributed_1_loss: 0.0495\n",
      "Epoch 2/10\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 0.1985 - dense_2_loss: 0.1724 - time_distributed_1_loss: 0.0261 - val_loss: 0.0317 - val_dense_2_loss: 0.0154 - val_time_distributed_1_loss: 0.0164\n",
      "Epoch 3/10\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.0221 - dense_2_loss: 0.0135 - time_distributed_1_loss: 0.0086 - val_loss: 0.0130 - val_dense_2_loss: 0.0083 - val_time_distributed_1_loss: 0.0047\n",
      "Epoch 4/10\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 0.0047 - dense_2_loss: 0.0018 - time_distributed_1_loss: 0.0029 - val_loss: 0.0060 - val_dense_2_loss: 0.0039 - val_time_distributed_1_loss: 0.0021\n",
      "Epoch 5/10\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 0.0031 - dense_2_loss: 0.0013 - time_distributed_1_loss: 0.0017 - val_loss: 0.0027 - val_dense_2_loss: 0.0012 - val_time_distributed_1_loss: 0.0015\n",
      "Epoch 6/10\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 0.0023 - dense_2_loss: 0.0010 - time_distributed_1_loss: 0.0013 - val_loss: 0.0021 - val_dense_2_loss: 9.6429e-04 - val_time_distributed_1_loss: 0.0012\n",
      "Epoch 7/10\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 0.0018 - dense_2_loss: 8.0420e-04 - time_distributed_1_loss: 0.0010 - val_loss: 0.0017 - val_dense_2_loss: 7.8879e-04 - val_time_distributed_1_loss: 9.4051e-04\n",
      "Epoch 8/10\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.0014 - dense_2_loss: 6.1163e-04 - time_distributed_1_loss: 8.2089e-04 - val_loss: 0.0014 - val_dense_2_loss: 6.7429e-04 - val_time_distributed_1_loss: 7.6175e-04\n",
      "Epoch 9/10\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.0012 - dense_2_loss: 5.2368e-04 - time_distributed_1_loss: 6.7365e-04 - val_loss: 0.0012 - val_dense_2_loss: 5.6423e-04 - val_time_distributed_1_loss: 6.3323e-04\n",
      "Epoch 10/10\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 0.0010 - dense_2_loss: 4.5388e-04 - time_distributed_1_loss: 5.6876e-04 - val_loss: 0.0010 - val_dense_2_loss: 4.6169e-04 - val_time_distributed_1_loss: 5.4272e-04\n"
     ]
    }
   ],
   "source": [
    "interpreter = build_interpreter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Frag_Temperatur',\n",
       " ['-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  'Zeit',\n",
       "  '-',\n",
       "  'Zeit',\n",
       "  '-',\n",
       "  'Ort',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter('Welche ungefähre Temperatur war 1992 und 2018 in Sachsen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Und jetzt kannt Du loslegen &mdash; der WetterBot kann noch nicht viel, ist aber nun recht einfach zu trainieren! Und mit der selbstgebauten Intent Recognition wird er bestimmt noch besser! Ein paar Ideen dazu gibt Dir das Notebook mit Aufgaben zu Intent Recognition.\n",
    "\n",
    "_Viel Spaß und bis bald zu einer neuen Lektion vom codecentric.AI bootcamp!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "nlp_intent.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "96px",
    "width": "250px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
