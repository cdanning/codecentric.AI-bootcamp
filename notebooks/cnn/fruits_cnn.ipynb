{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codecentric.AI Bootcamp - Convolutional Neural Networks\n",
    "\n",
    "Hallo und herzlich Willkommen zurück beim **codecentric.AI Bootcamp**.\n",
    "\n",
    "Heute geht es um die Frage: Wie lernen Computer eigentlich sehen? Also, wie lernen sie Bilder und Objekte auf Bildern zu erkennen?\n",
    "\n",
    "Eine der gängigsten Methoden dafür sind sogenannten **Convolutional Neural Networks**. Dieses Notebook enthält Beispiele und Übungsaufgaben.\n",
    "\n",
    "Eine theoretische Einführung in Convolutional Neural Networks gibt es in diesem [YouTube video](https://youtu.be/MWPohcMtFLo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lade Video\n",
    "from IPython.display import IFrame    \n",
    "IFrame('https://www.youtube.com/embed/MWPohcMtFLo', width=850, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu diesem Notebook gibt es ebenfalls ein [Video](https://youtu.be/), in dem ich euch durch dieses Beispiel durchführe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lade Video\n",
    "from IPython.display import IFrame    \n",
    "IFrame('https://www.youtube.com/embed/', width=850, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Networks\n",
    "\n",
    "Convolutional Neural Nets werden meisten als **CNNs** oder **ConvNets** abgekürzt; sie sind ein spezieller Typ von neuronalen Netzen, bei denen das Lernen etwas anders funktioniert, als bei Multi-Layer Perceptronen.\n",
    "\n",
    "Die Funktionsweise von CNNs kann man sich ein bisschen so vorstellen, wie das **rezeptive Feld von Fotorezeptoren** im menschlichen Auge. Rezeptive Felder im menschlichen Auge sind kleinere zusammenhängende Bereiche auf der Netzhaut, wo Gruppen von vielen Fotorezeptoren gemeinsam auf wenige Ganglienzellen wirken. Jede Ganglienzelle kann also von einer großen Gruppe an Rezeptonen stimuliert werden. Aus einem komplexen Input ensteht so ein komprimierter Output, der im Gehirn weiter verarbeitet wird.\n",
    "\n",
    "Genauer lernen CNNs, indem sie verschiedene **Abstraktionsebenen** der möglichen Klassen finden. Das heißt, in den ersten versteckten Schichten des neuronalen Netzes werden meistens allgemeine Muster, wie Kanten erkannt. Je weiter wir in den versteckten Schichten Richtung Output kommen, desto spezifischer werden die erkannten Muster, z.B. Texturen, grobe Muster, einzelne Teile der Objekte auf den Bildern und letztendlich ganze Objekte.\n",
    "\n",
    "Bei CNNs werden **Gruppen aus benachbarten Pixeln** betrachtet. In diesem Neuronalen Netz sind Neuronen zunächst nur mit den lokal entsprechenden Neuronen in den Folgeschichten verbunden; das bezeichnen wir als **\"local connectivity\"**. Dadurch kann das Netz effizient und relativ schnell den Kontext von Mustern und Objekten lernen und diese auch in anderen Postionen auf dem Bild erkennen. Konkret funktionert das mit sogenannten **Sliding Windows**, also Fenstern, die eine Gruppe von Pixeln betrachten und dabei von links oben nach rechts unten das Bild abscannen. Gelernt werden dabei dann auch keine Gewichte, sondern **Filter** oder **Kernel**.\n",
    "\n",
    "Auf jedem Fenster des Sliding Windows wird nämlich nun eine bestimmte mathematische Operation durchgeführt, die sogenannte **Faltung** - oder auf Englisch: **Convolution**. Diese Faltung passiert für jedes Fenster des gesamten Bildes. Die Faltung passiert über das Multiplizieren der Pixelwerte in unserem Fenster mit einem sogenannten Filter. Dieser Filter ist nichts anderes als eine Matrix mit den selben Dimensionen des Sliding Windows. Je nachdem, welche Werte in einem Filter stehen, führt die Faltung zu einer bestimmten **Transformation** des Originalbildes.\n",
    "\n",
    "## Bibliotheken\n",
    "\n",
    "Zunächst laden wir die grundlegenden Pakete, die wir für die Vorbereitung der Daten benötigten. Dazu gehören\n",
    "\n",
    "- **cv2**: OpenCV for image processing.\n",
    "- **numpy**: NumPy ist das wichtigste Paket für maschinelles Lernen in Python, denn es bietet die nötigen Funktionen für die Arbeit mit Matrizen und n-dimensionalen Arrays, linearer Algebra, und mehr.\n",
    "- **pandas**: pandas erleichtert das Arbeiten mit Daten in Python.\n",
    "- **random**: for pseudo-random number generation.\n",
    "- **matplotlib**: Zum Erstellen von Graphiken und Abbildungen aus unseren Daten nutzen wir matplotlib. Den zusätzlichen Befehl `matplotlib inline` geben wir in unserem Juypter Notebook mit, damit wir die generierten Plots unterhalb des Code-Chunks sehen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein CNN mit Keras und TensorFlow trainieren\n",
    "\n",
    "In dieser Lektion verwenden wir [**Keras** und TensorFlow](https://keras.io/) um ein CNN zu trainieren, das Bilder von verschiedenen Früchten erkennen kann.\n",
    "\n",
    "Dafür laden wir zunächst die nötigen Funktionen aus dem `keras` Paket. Was diese im einzelnen bedeuten und tun, erkläre ich, wenn wir sie verwenden.\n",
    "\n",
    "Per Default wird mit Keras das **TensorFlow backend für CPUs** verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image processing\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# build your own nets\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten\n",
    "\n",
    "Die Daten (also die Bilder), die wir verwenden, sind von [Kaggle](https://www.kaggle.com/moltean/fruits/data) und zeigen verschiedene Früchte auf weißem Hintergrund (Copyright (c) 2017 Mihai Oltean, Horea Muresan, MIT License). Einen Teil dieser Früchte haben wir euch in den `data` Ordner gelegt, damit ihr direkt loslegen könnt. Diese Bilder sind in dem Ordner `fruits-360` gespeichert, der zwei Unterordner enthält: `Training` und `Test`. Beide Unterordner enthalten wieder Unterordner mit den Namen der einzelnen Früchte. Hier wollen wir die **Trainingsbilder** zu trainieren des CNNs verwenden und die Testbilder zum validieren. Entsprechend definieren wir die Pfade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files_path = \"/data/fruits-360/Training/\"\n",
    "valid_image_files_path = \"/data/fruits-360/Test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun definieren wir noch eine Reihe von anderen Objekten und Hyperparametern. Das tun wir in einem Block direkt am Anfang, damit wir später, wenn wir etwas ändern möchten, nicht unser gesamtes Notebook durchsuchen müssen.\n",
    "\n",
    "- `fruit_list`: Diese Liste enthält hier alle **Namen** der jeweiligen Unterordner in `Training` und `Test`. Wir werden diese Liste gleich zum einlesen der Bilder verwenden. Wir könnten hier auch nur einen Teil der Ordner verwenden.\n",
    "- `output_n`: Anzahl der **Klassen** (also der zu klassifizierenden Früchte).\n",
    "- `img_width` und `img_height`: **Größe** der Bilder. Die Originalbilder sind 100 x 100 Pixel groß; wir werden die hier auf 20 x 20 runterskalieren. Keras verwendet dafür die `PIL.Image.resize()` Funktion.\n",
    "- `channels`: Anzahl der **Farbkanäle**. Hier 3, da wir Farbbilder mit RGB-Kanälen haben.\n",
    "- `batch_size`: Definiert die Anzahl der Bilder, die gemeinsam den Optimierungs- + Backpropagation-Prozess durchlaufen. Dadurch wird die Berechnung schneller aber eine zu große **Batch-Größe** kann den Lernprozess verschlechtern.\n",
    "- `epochs`: Anzahl der Runden (**Epochen**), die unser CNN trainiert werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_list = [\"Kiwi\", \"Banana\", \"Apricot\", \"Avocado\", \"Cocos\", \"Clementine\", \"Mandarine\", \"Orange\",\n",
    "              \"Limes\", \"Lemon\", \"Peach\", \"Plum\", \"Raspberry\", \"Strawberry\", \"Pineapple\", \"Pomegranate\"]\n",
    "\n",
    "output_n = len(fruit_list)\n",
    "img_width = 20\n",
    "img_height = 20\n",
    "channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gucken wir uns aber zuerst mal ein paar der Bilder an. Dafür generieren wir eine Liste mit allen Bildern in unserem Trainings-Ordner und wählen zufällig 24 Stück aus, die wir mit `matplotlib` plotten können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beispiel_bilder = !find $train_image_files_path -type f -name \"*.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beispiel_bilder = 24\n",
    "beispiel_bilder = random.sample(beispiel_bilder, num_beispiel_bilder)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "for i in range(num_beispiel_bilder):\n",
    "    fig.add_subplot(4, 6, i + 1)\n",
    "    plt.axis('off')\n",
    "    img = plt.imread(beispiel_bilder[i])\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilder mit Keras einlesen\n",
    "\n",
    "Um die Bilder einzulesen verwenden wir zwei praktische Funktionen aus `keras`, die genau für den Fall gemacht sind, dass wir Bilder in Unterordnern sortiert haben und die Namen der Unterordner die Klassen-Label darstellen:\n",
    "\n",
    "- `ImageDataGenerator`: generiert **Batches von Bilddaten**, optional mit Data Augmentation (Multiplizierung und Veränderung der Trainingsbilder). Hier **normalisieren** wir die rohen Pixelwerte, indem wir durch den Maximalwert 255 teilen und so Werte zwischen 0 und 1 erhalten.\n",
    "- `flow_from_directory`: **liest Bilder aus Dateien** entsprechend dem definierten ein `ImageDataGenerator`, optional mit Skalierung auf eine `target_size`. Lest euch die Hilfe für die Funktion durch (`?train_data_gen.flow_from_directory`), um mehr über die anderen Argumente zu erfahren.\n",
    "\n",
    "Diese beiden Funktionen wenden wir entsprechend auf Trainings- und Validierungsdaten an (Achtung: Data Augmentation NICHT auf Validierungsdaten anwenden).\n",
    "\n",
    "Den `ImageDataGenerator` könnten wir auch verwenden, um Kreuzvalidierungsdaten aus nur einem Trainingsset zu definieren. Dafür würden wir das Argument `validation_split` in Kombination mit `flow_from_directory(subset = 'training or validation')` verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale = 1 / 255\n",
    ")\n",
    "\n",
    "valid_data_gen = ImageDataGenerator(\n",
    "    rescale = 1 / 255\n",
    ")\n",
    "\n",
    "train_image_array_gen = train_data_gen.flow_from_directory(\n",
    "    train_image_files_path,\n",
    "    target_size = (img_width, img_height),\n",
    "    class_mode = 'categorical',\n",
    "    classes = fruit_list,\n",
    "    color_mode = 'rgb', \n",
    "    batch_size = batch_size,\n",
    "    seed = 42)\n",
    "\n",
    "valid_image_array_gen = valid_data_gen.flow_from_directory(\n",
    "    valid_image_files_path,\n",
    "    target_size = (img_width, img_height),\n",
    "    class_mode = 'categorical',\n",
    "    classes = fruit_list,\n",
    "    color_mode = 'rgb', \n",
    "    batch_size = batch_size,\n",
    "    seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Keras-Modell erstellen\n",
    "\n",
    "In diesem Keras-Beispiel verwenden wir die einfachere sequentielle API (im Gegensatz zur etwas komplexeren aber flexibleren funktionellen API), deshalb müssen wir unser Modell zunächst mit der `Sequential()` Funktion initialisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Input-Schicht, in die unsere Bilder mit ihren Pixel-Arrays eingehen (und die in Keras implizit ist, also nicht speziell definiert werden muss, sondern deren Größe mit `input_shape = (img_width, img_height, channels))` in der ersten versteckten Schicht definiert wird), kommen in der Regel eine oder zwei **Faltungsschichten** (plus **Aktivierungsfunktion** nach jeder Faltung, z.B. Rectified Linear Units `relu`). \n",
    "\n",
    "Die erste versteckte Schicht in unserem Keras CNN definieren wir nun mit der `Conv2D()` Funktion.\n",
    "\n",
    "Der erste Hyperparameter in dieser Funktion ist `filter`. Manchmal werden **Filter** auch als **Kernel** oder Filter Kernel bezeichnet. Tatsächlich sind Filter Sammlungen von Kernels, d.h. wenn wir mit Farbbildern und 3 Kanälen - also 3 Dimensionen - arbeiten, haben wir einen Kernel pro Kanal, aus dem sich ein Filter zusammensetzt. Pro Filter bekommen wir dann EINEN Ergebniswert aus dem Skalarprodukt zurück!\n",
    "\n",
    "Beispiele für Filter können zum Beispiel das Originalbild verwaschen, untere oder obere horizontale Kanten erkennen. Im Prinzip können aber beliebige Werte in die Filter eingesetzt werden, so dass verschiedenste Muster in dem Bild hervortreten. Jedes Muster stellt eine sogenannte **Feature Map** oder **Activation Map** dar. Die Werte, die an den einzelnen Stellen des Filters stehen, sollen jetzt von unserem neuronalen Netz gelernt werden. Das CNN lernt also, welche Transformation es wann durchführen muss, um die richtigen Muster und Objekte in den Bildern zu erkennen. Dafür lernt das CNN nicht nur einen Filter, sondern sehr sehr viele. Es lernt sogar in jeder versteckten SCHICHT mehrere Filter parallel - in unserer ersten versteckten Schicht haben wir `filters = 32` angegeben, d.h. dass unser CNN in dieser Schicht 32 Filter parallel lernt!\n",
    "\n",
    "Diese unabhängig voneinander parallel gelernten Filter, bzw. die transformierten Output-Bilder nach der Faltung, produzieren die sogenannten **Stacks of Feature Maps** oder Activation Maps. Die Anzahl der zu trainierenden Parameter ist bei Faltungen deutlich geringer als bei voll verknüpften Neuronen in MLPs. Bei Farbbildern mit 3 Farbkanälen haben wir 3-dimensionale Filter, die das Ergbenis ebenfalls mit dem Skalarprodukt aus Filter und Bildausschnitt berechnen.\n",
    "\n",
    "Die `kernel_size` definiert die **Größe des Sliding Window** (also hier 3 x 3 Pixel).\n",
    "\n",
    "**Schrittgröße** `strides` bezeichnet, wie weit das Sliding Window bei jedem Schritt vorrückt. Meisten wird eine Schrittgröße von 1 verwendet, d.h. dass das Sliding Window nach jeder Faltung jeweils einen Pixel weiterwandert - und zwar sowohl von links nach rechts, als auch von oben nach unten (darum `strides = (1, 1)`). Würden wir die Schrittgröße vergrößern, würde die Berechnung des neuronalen Netzes schneller gehen, wir würden aber auch nicht so detaillierte Muster erkennen können. Und unser Bild würde sich im Output verkleinern.\n",
    "\n",
    "**Padding** bezeichnet das Hinzufügen von extra Pixelschichten am Rand des Bildes, damit jeder Pixel des eigentlichen Bildes beim scannen des Sliding Windows gleich häufig gefaltet wird. Würden wir kein Padding anwenden, würden die Randpixel weniger häufig betrachtet, als die Pixel in der Mitte des Bildes - und unser Bild würde nach der Transformation kleiner werden. Es gibt mehrere Arten von Padding. Bei `padding = \"same\"` werden die Randpixel dupliziert und an an den Rand hinzugefügt. Alternativ könnten wir die Ränder auch mit Nullern auffüllen, dem sogenannten Zero-Padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first hidden layer\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3, 3),\n",
    "                 strides = (1, 1),\n",
    "                 padding = \"same\", \n",
    "                 input_shape = (img_width, img_height, channels)))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der zweiten versteckten Schicht definieren wir hier eine zweite Faltungsschicht, dieses Mal mit 16 Filtern.\n",
    "\n",
    "Auch die Aktivierungsfunktion verändern wir hier ein wenig, und zwar mit der **Leaky Rectified Linear Units** Funktion `LeakyReLU`. Normale ReLU-Funktionen können \"sterben\" wenn wir viele negativen Werte haben, da alle negativen Werte einheitlich auf 0 gesetzt werden und so kein Gradient mehr vorhanden ist. D.h. der Gradient ist ebenfalls 0, und liefert keine Information über den nächsten Schritt in der Optimierung. Bei Leaky ReLU werden auch negative Werte transformiert, so dass dieses Problem nicht mehr auftreten kann.\n",
    "\n",
    "Außerdem verwenden wir an dieser Stelle **Batch-Normalisierung**, die die Ergebnisse aus der zweiten Faltungsschicht normalisiert. Da jeder Batch eine Approximation der tatsächlichen Verteilung der Daten darstellt, kann es in den Batches leicht zu Veränderungen in der Verteilung kommen. Das kann zu Problemen führen und Batch-Normalisierung kann hier helfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second hidden layer\n",
    "model.add(Conv2D(16, (3, 3), padding = \"same\"))\n",
    "model.add(LeakyReLU(alpha = 0.5))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN-Architekturen bestehen aber nicht nur aus Schichten, in denen Faltung passiert, sondern sie haben zusätzlich sogenannte **Pooling**-Schichten. In den Pooling-Schichten werden die Bilder (also die transformierten Outputs aus vorhergehenden Faltungsschichten) verkleinert. Dafür wird auch wieder ein Sliding Window verwendet, dieses muss nicht die selbe Größe haben, wie das Sliding Window aus den Faltungsschichten, hier verwenden wir `pool_size = (2,2)`. Wichtig ist dabei, dass das Sliding Window beim Pooling meist nicht überlappt, sondern jeder Pixel genau 1x betrachtet wird, damit das Output-Bild entsprechend kleiner wird.\n",
    "\n",
    "Es gibt mehrere Arten, wie Bilder mit Pooling verkleinert werden können. Die häufigsten sind **Max Pooling**, wo nur der größte Wert jedes Fensters behalten wird, Average Pooling, wo der Durschschnittswert aus jedem Fenster gebildet wird und Sum Pooling, wo die Summe aller Werte gebildet wird. Hier verwendet wir `MaxPooling2D` plus `Dropout` (zufällige Inaktivierung von 25% der Neuronen). Pooling arbeitet dabei unabhängig auf jeder einzelnen Feature Map und sorgt nicht nur für eine Reduktion des Bildes und damit an zu optimierenden Parametern, sondern hilft auch dabei, allgemeingültige Feature zu extrahieren, die robust gegenüber kleineren Änderungen des Inputs sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max pooling\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir das Endergbnis, zum Beispiel eine Klasse bestimmen lassen, folgt ein **Dense-Layer** mit 512 Knoten (`Dense(512)`), als würden wir ein kleines MLP hinten dran hängen. Damit unsere multidimensionalen Filter in den Dense-Layer übergehen können, müssen wir die Daten erst \"`Flatten()`\". Im Dense-Layer werden deshab wieder Gewichte gelernt. Im Wesentlichen haben die Faltungsschichten in unserem Netz relevante Feature gefunden, und die Pooling-Schichten habe diese Information soweit verdichtet, dass es nun sinnvoll ist ein MLP zu trainieren, dass in der Lage ist die finale Klassifikation zu lernen.\n",
    "\n",
    "Und zuletzt haben wir eine **Output-Schicht** mit der Anzahl der möglichen Klassen (`Dense(output_n)`) und die Softmax Aktivierungsfunktion, ebenfalls wie bei einem einfachen MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten max filtered output into feature vector \n",
    "# and feed into dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Outputs from dense layer are projected onto output layer\n",
    "model.add(Dense(output_n))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Später werden wir die Anzahl der Trainings- und Validierungsbilder benötigen. Damit wir leicht damit arbeiten können, definieren wir diese Zahlen hier als Objekte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_image_array_gen.n\n",
    "valid_samples = valid_image_array_gen.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir das neuronale Netz wie oben beschrieben definiert haben, müssen wir es **kompilieren**. Dafür geben wir die folgenden drei Argumente an:\n",
    "\n",
    "- `loss`: die zu minimierende [Loss-Funktion](https://keras.io/losses/), hier kategorische Kreuzentropie\n",
    "- `optimizer`: [Optimierungsfunktion](https://keras.io/optimizers/), hier RMSprop, eine ähnliche Technik wie Gradient Descent mit Momentum\n",
    "- `metrics`: [Performance-Metrik(en)](https://keras.io/metrics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = RMSprop(lr = 0.0001, decay = 1e-6),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann das Modell trainiert werden. Da wir unsere Daten mit dem ImageDataGenerator einlesen wollen, verwenden wir hier analog die `fit_generator` Funktion und geben die Trainingsdaten, Validierungsdaten, Anzahl der Epochen und die Anzahl der Schritte (`steps_per_epoch`) an, um die Anzahl der einzulesenen Bilder pro Batch zu definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_image_array_gen,\n",
    "    steps_per_epoch = int(train_samples / batch_size), \n",
    "    epochs = epochs, \n",
    "    validation_data = valid_image_array_gen,\n",
    "    validation_steps = int(valid_samples / batch_size),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Output während des Trainings gibt uns schon einen Überblick über die Entwicklung der Performance-Metrik auf Trainings- und Validierungsdaten in den einzelnen Epochen. Wir können das ganze aber auch mit Matplotlib plotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorhersage auf Testdaten\n",
    "\n",
    "Und letztendlich können wir das so trainierte Modell nun für Vorhersagen auf neuen Testdaten verwenden, z.B. auf diesem Bild einer Banane, das von Wikipedia heruntergeladen wird:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/commons/8/8a/Banana-Single.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das heruntergeladene Testbild ist im Ordner `test_images` gespeichert und kann entweder mit dem ImageDataGenerator eingelesen werden (macht vor allem Sinn, wenn wir viele Testbilder haben) oder wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_files_path = \"test_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = !find $test_image_files_path -type f -name \"*.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir Vorhersagen mit unserem Modell machen, definieren wir noch eine Liste mit den Klassen-Indizes, denn Keras wird uns keine Label im Sinne von Strings mit den Namen der Früchte zurück geben sondern Indizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_image_array_gen.class_indices\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir alles für die Vorhersage in eine Funktion verpacken, die\n",
    "\n",
    "- das zu klassifizierende Bild einliest,\n",
    "- plottet,\n",
    "- in die richtigen Dimensionen für Keras bringt (da Keras immer eine Batch-Dimension erwartet, fügen wir diese mit der `expand_dims` Funktion hinzu),\n",
    "- skaliert\n",
    "- und schließlich klassifiziert.\n",
    "\n",
    "Die eigentliche Klassifikation erfolgt mit der `predict` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_model(image, classes=classes):\n",
    "    img = cv2.imread(image)        \n",
    "    b,g,r = cv2.split(img)       # get b,g,r\n",
    "    img = cv2.merge([r,g,b])     # switch it to rgb\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "    image = image_utils.load_img(image, target_size=(img_width, img_height))\n",
    "    image = image_utils.img_to_array(image)\n",
    "\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # scale pixels between 0 and 1, sample-wise\n",
    "    image /= 255.\n",
    "        \n",
    "    prediction = model.predict(image)\n",
    "    \n",
    "    pred = prediction.argmax()\n",
    "\n",
    "    for k, v in classes.items():\n",
    "        if (v == pred):\n",
    "            pred_label = k\n",
    "        \n",
    "    proba = prediction.max()\n",
    "    \n",
    "    print(\"Predicted class: \" + pred_label + \" with probability \" + str(proba*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenden wir unsere Vorhersagefunktion also auf das Testbild an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_image_model(test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser Modell liegt richtig!\n",
    "\n",
    "Noch ein Hinweis zum Abschluss: Wir könnten Testbilder auch mit der `predict_generator()` und `evaluate_generator()` vorhersagen/bewerten. Das ist hier nicht gezeigt, kann aber einfach analog zu den Trainingsdaten mit `ImageDataGenerator` und `flow_from_directory` implementiert werden - geändert werden müsste dafür `class_mode = 'categorical'` in `class_mode = 'None'`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
