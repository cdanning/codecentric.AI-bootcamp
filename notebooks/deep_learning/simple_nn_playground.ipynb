{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grundlagen Neuronale Netze - einfache Operationen\n",
    "\n",
    "https://bootcamp.codecentric.ai\n",
    "\n",
    "In diesem Notebook wollen wir das einfache Beispiel aus dem Video nachvollziehen. \n",
    "\n",
    "Wir:\n",
    "- definieren einen Input Tensor (aus einem 28x28 Pixel Bild von MNIST)\n",
    "- wir normalisieren die Werte des Bildes\n",
    "- wir definieren Matrizen mit Gewichten, die wir lernen wollen\n",
    "- wir kombinieren Matrix Multiplikationen und Aktivierungsfunktionen, um aus einem Input mit 784 Pixeln einen Output mit 10 Werten zu erhalten\n",
    "- wir definieren ein Label\n",
    "- (wir optimieren die Gewichte, damit sie zum Label passen - kleiner Vorausblick auf kommende Videos)\n",
    "\n",
    "Das folgende neuronale Netz ist keine besonders sinnvolle Architektur. Auch das Training mit nur einem Bild macht natürlich wenig Sinn. Es geht darum zu verstehen, welche Rechenoperationen \"unter der Haube\" eines neuronalen Netzes stattfinden. **Daher ist das ganze (hier) noch stark vereinfacht.**\n",
    "\n",
    "Hier noch einmal das Bild, was wir versuchen in Code nachzuvollziehen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simple nn](simple_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel mit PyTorch\n",
    "\n",
    "Zunächst ein paar benötigte Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgender Tensor ist die interne Darstellung eines Bildes.\n",
    "\n",
    "Es ist ein 28x28 Pixel Matrix mit Zahlenwerten von 0-255. \n",
    "\n",
    "(0 = schwarz, 255 = weiss, dazwischen Graustufen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.tensor(\n",
    "       [[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
    "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
    "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
    "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
    "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
    "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
    "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
    "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
    "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
    "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
    "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
    "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
    "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
    "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
    "       dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So kann man sich die Dimensionen des Tensors anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und so sieht es aus, wenn man die Zahlen als Bild interpretiert (eine Zahl 5 aus dem MNIST Datensatz):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2d3b4b37f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt klopfen wir den Tensor flach. Aus einer 28 x 28 Matrix wird ein Vektor mit 784 \"Input Pixeln\". Das sind die gleichen Zahlenwerte - nur nicht mehr in 28 Reihen sondern alle in einer Reihe aneinander gehängt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = img.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt schauen wir uns mal den Wert an Stelle 180 an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(170.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Wert beträgt 170 ...\n",
    "\n",
    "... nun schauen wir an was der größte Wert in dem Vektor ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(255.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie zu erwarten war, ist es 255 (weiß - sicher sind einige Pixel in dem Bild weiß - größere Zahlen kann es bei einem solchen Bild nicht geben).\n",
    "\n",
    "Jetzt machen wir eine einfache \"Normalisierung\" und teilen alle Werte des Vektors durch 255.\n",
    "\n",
    "Damit ändern wir den Zahlenbereich im Vektor von 0-255 auf 0-1. Mit diesem Schritt kann man Probleme beim Training verringern - vor allem bei tieferen neuronalen Netzen wird das sehr wichtig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_input_tensor = input_tensor / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Wert an der Stelle 180 (den wir vorher schon angesehen haben) ist jetzt 0.6667\n",
    "\n",
    "Die Zahlen stehen aber noch im gleichen Verhältnis 0,66 ist 2/3 von 1 sowie 170 2/3 von 255 ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_input_tensor[180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(normalized_input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie zu erwarten ist die größte Zahl im Vektor jetzt 1\n",
    "\n",
    "Jetzt initialisieren wir unsere erste Weight Matrix mit Parametern, die gelernt werden können. Anders als im Video wählen wir nicht 784x3 sondern 784x20 - im Video wurde nur eine kleinere Zahl gewählt, damit es auf eine Folie passt und übersichtlicher aussieht.\n",
    "\n",
    "Die Zahlen sind zunächst (kleine) Zufallszahlen. (was requires_grad bedeutet überspringen wir an dieser Stelle - dazu kommen wir später)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0656,  0.0342, -0.0048,  ...,  0.0504, -0.0240, -0.0235],\n",
       "        [-0.0566,  0.0155,  0.0491,  ...,  0.0247, -0.0079,  0.0358],\n",
       "        [-0.0425, -0.0498,  0.0020,  ..., -0.0059, -0.0416, -0.0090],\n",
       "        ...,\n",
       "        [-0.0171, -0.0070,  0.0356,  ...,  0.0129,  0.0382,  0.0459],\n",
       "        [-0.0407, -0.0839, -0.0490,  ..., -0.0090,  0.0100,  0.0325],\n",
       "        [-0.0334,  0.0194, -0.0490,  ...,  0.0641, -0.0237,  0.0271]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_tensor = torch.randn((784, 20)) / math.sqrt(784)\n",
    "weights_tensor.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 20])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt berechnen wir wie im Video zuvor einige \"Activations\". Dazu machen wir eine Matrix-Multiplikation mit dem Input @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_activation = normalized_input_tensor @ weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4316,  0.1518,  0.4564, -0.0504, -0.0354,  0.4931,  0.4556, -0.1883,\n",
       "        -0.1547,  0.1315,  0.1643, -0.2995,  0.8672, -0.3522, -0.3659,  0.1599,\n",
       "         0.1575, -0.2205, -0.2636,  0.0573], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist das Ergebnis unserer ersten Matrix-Multiplikation.\n",
    "\n",
    "Anders als im Video Beispiel hat diese jetzt auch wieder eine Size von 20, da wir ja eine größere Weight Matrix gewählt haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_activation.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt kommt die Aktivierungs-Funktion, um auch nicht lineare Zusammenhänge lernen zu können. \n",
    "Im Prinzip setzt diese alle negativen Activations aus dem vorigen Schritt auf 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_activation = first_activation.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1518, 0.4564, 0.0000, 0.0000, 0.4931, 0.4556, 0.0000, 0.0000,\n",
       "        0.1315, 0.1643, 0.0000, 0.8672, 0.0000, 0.0000, 0.1599, 0.1575, 0.0000,\n",
       "        0.0000, 0.0573], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt initialisieren wir die zweite Weight Matrix (wie im Video Beispiel). Hier müssen wir jetzt auch wieder die Size von 3 auf 20 anpassen, damit die Matrix-Multiplikationen zusammen passen. Nach wie vor wollen wir aber eine Output Größe von 10 haben (in unserem Beispiel wollen wir ja Zahlen von 0-9) vorhersagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0797, -0.1833, -0.5184,  0.0186, -0.3187, -0.0950, -0.0871,  0.0958,\n",
       "          0.2110, -0.0855],\n",
       "        [-0.0806, -0.0119,  0.0253, -0.2516, -0.1614, -0.0086,  0.1103,  0.1938,\n",
       "         -0.0857, -0.5184],\n",
       "        [ 0.0609,  0.2804, -0.1119, -0.3617,  0.5554, -0.0527, -0.2334,  0.0757,\n",
       "          0.1637,  0.0369],\n",
       "        [-0.1400, -0.3254,  0.2593, -0.1850, -0.3902,  0.3106, -0.3367, -0.0212,\n",
       "          0.1835,  0.0692],\n",
       "        [-0.0138, -0.0180,  0.0535, -0.1926,  0.0657,  0.0492,  0.0712,  0.0954,\n",
       "          0.4357,  0.1249],\n",
       "        [ 0.1178,  0.1734,  0.0588,  0.4374,  0.1805,  0.0251, -0.3083,  0.3585,\n",
       "         -0.1906,  0.5671],\n",
       "        [ 0.2936, -0.1956, -0.1607, -0.1308,  0.2795,  0.0351,  0.3279, -0.0726,\n",
       "         -0.1633,  0.0675],\n",
       "        [-0.0543, -0.2475,  0.0963,  0.2062,  0.0639, -0.4213,  0.0014,  0.2473,\n",
       "         -0.1331,  0.0445],\n",
       "        [ 0.3618,  0.0155,  0.1939,  0.3116, -0.0055, -0.0990,  0.2023, -0.3053,\n",
       "          0.0453, -0.1513],\n",
       "        [-0.1117, -0.1513,  0.2402,  0.0165,  0.0012, -0.2909, -0.1257, -0.1912,\n",
       "          0.2388,  0.0893],\n",
       "        [ 0.4973, -0.3279,  0.0113, -0.2572, -0.1315, -0.2861,  0.2245, -0.1384,\n",
       "          0.1742, -0.2922],\n",
       "        [-0.0138,  0.1288,  0.1627,  0.5239, -0.2764, -0.1166,  0.1701,  0.1142,\n",
       "         -0.5387, -0.1152],\n",
       "        [ 0.2145, -0.2958,  0.5686,  0.2603,  0.2303,  0.1363, -0.0750,  0.4154,\n",
       "         -0.0903,  0.0974],\n",
       "        [-0.1339,  0.3034, -0.5628, -0.2630, -0.0051,  0.1122,  0.0029,  0.2304,\n",
       "          0.0076,  0.4001],\n",
       "        [-0.2460,  0.0655,  0.2301, -0.2488,  0.1406,  0.0080,  0.1952, -0.0562,\n",
       "          0.0259,  0.1324],\n",
       "        [-0.1654,  0.2169, -0.2001,  0.3005, -0.0349, -0.1606, -0.1090,  0.2920,\n",
       "          0.3109,  0.4034],\n",
       "        [-0.0201, -0.3436,  0.1618,  0.1132, -0.2436,  0.3214, -0.0940, -0.0374,\n",
       "          0.1230, -0.2113],\n",
       "        [ 0.2946, -0.1975,  0.2336, -0.0326, -0.1715,  0.2342, -0.0289,  0.1614,\n",
       "         -0.4630,  0.0036],\n",
       "        [-0.1466,  0.7025, -0.0187,  0.3739,  0.0609,  0.1060, -0.0730,  0.0636,\n",
       "          0.2115, -0.0766],\n",
       "        [ 0.3502,  0.2071, -0.0529,  0.3347, -0.2059, -0.1999,  0.3175,  0.1341,\n",
       "         -0.0495, -0.0664]], requires_grad=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_weights_tensor = torch.randn((20, 10)) / math.sqrt(20)\n",
    "more_weights_tensor.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... eine weitere Matrix-Multiplikation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = second_activation @ more_weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4509, -0.2152,  0.4255,  0.2235,  0.5679,  0.0494, -0.1512,  0.5685,\n",
       "        -0.0587,  0.3241], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und das ist jetzt erstmal unser Output. Der Vektor hat die richtige Dimension. Die Zahlen darin sind bis hierhin erstmal **völlig bedeutungslos**.\n",
    "\n",
    "Wir haben einmal das Modell mit Zufallszahlen durchgerechnet und geschaut, was am Ende raus kommt -> Zufallszahlen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label definieren\n",
    "\n",
    "An dieser Stelle definieren wir jetzt ein Label. Das Label ist das was wir vom neuronalen Netz erwarten. \n",
    "\n",
    "Wenn ich vorne ein Bild einer 5 rein gebe, dann soll folgendes herauskommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese verbreitete Form eines Labels nennt man auch \"one hot encoded vector\". Es ist Vektor der verschiedene Klassen abbilden kann - der Vektor an der Stelle 0 ist die Wahrscheinlichkeit für eine 0. Der Vector an der Stelle 5 ist die Wahrscheinlichkeit für eine 5 etc. (Es könnte aber auch etwas völlig anderes bedeuten - z.B. Stelle 0 = Katze, Stelle 3 = Hund etc.)\n",
    "\n",
    "Da wir ja ein Bild einer 5 betrachten, soll also das label[5] = 1 und alles andere 0 sein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt fügen wir dem Output noch eine weitere Aktivierungs-Funktion hinzu und haben unsere \"prediction\" - also unsere Vorhersage. Diese bringt die Zahlen Werte in einen Bereich von 0-1 (warum ist an dieser Stelle nicht relevant - wir tun es einfach :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = output.sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier jetzt die aktuelle Vorhersage des Modells (gerundet, dass man es besser lesen kann):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 1., 1., 0., 1., 0., 1.], grad_fn=<RoundBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktion\n",
    "\n",
    "Unsere Loss Funktion soll uns den Fehler zwischen unserer prediction und dem label berechnen. Wir verwenden eine bestehende pytorch Funktion. (Warum genau diese, ist an dieser Stelle auch noch nicht relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.functional.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun berechnen wir einmal beispielhaft den aktuellen Loss, also den Fehler oder den \"Abstand\" zwischen unserer prediction und dem label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8130, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(prediction, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist unser loss - was sagt uns das? Erstmal noch gar nichts (auch hier sind wir immer bei völlig aussagslosen Zufallszahlen.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt machen wir eine \"manuelle Vorhersage\". Wir definieren einfach eine prediction wie sie uns gefällt. Sind bei dieser prediction mehr Einsen und Nullen an der richtigen Stelle sollte der folgende Loss kleiner weren - ansonsten größer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_prediction = torch.tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5262)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(manual_prediction, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was passiert wenn alle Zahlen richtig vorhergesagt werden? Wenn unsere prediction gleich dem label ist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(label, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... der Loss geht gegen 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell optimieren\n",
    "\n",
    "(Kleiner Vorausblick)\n",
    "\n",
    "Jetzt optimieren wir in ein paar Schritten unsere weight so, dass die prediction möglichst nah an das label heran kommt. Das ist das (vereinfachte) Prinzip, wie neuronale Netze lernen. Wir werden es in einem folgenden Video noch genauer betrachten.\n",
    "\n",
    "Daher gehe ich einfach die Schritte durch, ohne diese detailliert zu erkläeren.\n",
    "\n",
    "Zunächst berechnen wir den Loss als Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8130, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_func(prediction, label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir fordern pytorch dazu auf eine \"Backpropagation\" zu machen und die Gradienten für die weight Matrizen zu ermitteln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0093,  0.0068,  0.0092,  0.0084,  0.0097, -0.0074,  0.0070,  0.0097,\n",
       "          0.0074,  0.0088],\n",
       "        [ 0.0279,  0.0204,  0.0276,  0.0254,  0.0291, -0.0223,  0.0211,  0.0291,\n",
       "          0.0222,  0.0265],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0301,  0.0220,  0.0298,  0.0274,  0.0315, -0.0240,  0.0228,  0.0315,\n",
       "          0.0239,  0.0286],\n",
       "        [ 0.0278,  0.0203,  0.0276,  0.0253,  0.0291, -0.0222,  0.0211,  0.0291,\n",
       "          0.0221,  0.0264],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0080,  0.0059,  0.0080,  0.0073,  0.0084, -0.0064,  0.0061,  0.0084,\n",
       "          0.0064,  0.0076],\n",
       "        [ 0.0100,  0.0073,  0.0099,  0.0091,  0.0105, -0.0080,  0.0076,  0.0105,\n",
       "          0.0080,  0.0095],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0530,  0.0387,  0.0524,  0.0482,  0.0554, -0.0423,  0.0401,  0.0554,\n",
       "          0.0421,  0.0503],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0098,  0.0071,  0.0097,  0.0089,  0.0102, -0.0078,  0.0074,  0.0102,\n",
       "          0.0078,  0.0093],\n",
       "        [ 0.0096,  0.0070,  0.0095,  0.0087,  0.0101, -0.0077,  0.0073,  0.0101,\n",
       "          0.0076,  0.0091],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0035,  0.0026,  0.0035,  0.0032,  0.0037, -0.0028,  0.0026,  0.0037,\n",
       "          0.0028,  0.0033]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_weights_tensor.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definieren\n",
    "\n",
    "Hier definieren wir einfach nochmal die gleichen Berechnungen wir zuvor - nur in einer Funktion, so dass wir sie in einer Schleife immer wieder aufrufen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return ( ((x @ weights_tensor).relu()) @ more_weights_tensor).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6108, 0.4464, 0.6048, 0.5556, 0.6383, 0.5123, 0.4623, 0.6384, 0.4853,\n",
       "        0.5803], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(normalized_input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir machen eine prediction mit dem aktuellen Modell für unseren input Tensor (das Bild der 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6108, 0.4464, 0.6048, 0.5556, 0.6383, 0.5123, 0.4623, 0.6384, 0.4853,\n",
       "        0.5803], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = model(normalized_input_tensor)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... ermitteln den loss (als den Fehler zwischen Vorhersage und label) und machen eine backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(new_pred, label)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt verwenden wir die Gradienten, um die Weights ein kleines bisschen in die richtige Richtung zu optimieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "with torch.no_grad():\n",
    "    weights_tensor -= weights_tensor.grad * lr\n",
    "    more_weights_tensor -= more_weights_tensor.grad * lr\n",
    "    weights_tensor.grad.zero_()\n",
    "    more_weights_tensor.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6261, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = model(normalized_input_tensor)\n",
    "loss_func(new_pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... und wir sehen, dass unser Fehler tatsächlich etwas kleiner geworden ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterative Optimierung\n",
    "\n",
    "Wenn wir diese einfachen Optimierungsschritte jetzt ganz oft aufrufen, dann werden die Gewichte immer mehr so angepasst, dass sich die prediction immer mehr dem label annähert (der loss kleiner wird):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.6261407136917114\n",
      "Loss:  0.004133186303079128\n",
      "Loss:  0.0016082196962088346\n",
      "Loss:  0.0009595596930012107\n",
      "Loss:  0.0006722664693370461\n",
      "Loss:  0.0005125728785060346\n",
      "Loss:  0.0004117612261325121\n",
      "Loss:  0.0003426993207540363\n",
      "Loss:  0.0002926323504652828\n",
      "Loss:  0.00025476646260358393\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "for i in range(1000):\n",
    "    new_pred = model(normalized_input_tensor)\n",
    "    loss = loss_func(new_pred, label)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        weights_tensor -= weights_tensor.grad * lr\n",
    "        more_weights_tensor -= more_weights_tensor.grad * lr\n",
    "        weights_tensor.grad.zero_()\n",
    "        more_weights_tensor.grad.zero_()\n",
    "\n",
    "    if (i % 100 == 0): print(\"Loss: \", loss_func(new_pred, label).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Optimierung sieht unser Vorhersage nun so aus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4686e-04, 2.1183e-04, 2.6229e-04, 2.4961e-04, 2.0450e-04, 9.9978e-01,\n",
       "        2.5082e-04, 2.5847e-04, 2.5807e-04, 8.9158e-05],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(normalized_input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dieser Schreibweise ist auf einen ersten Blick erstmal nicht viel anzufangen. Schauen wir uns an welche Zahl am größten ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(normalized_input_tensor).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Zahl an der Stelle 5 ist die Größte. Also die 5, die wir vorhersagen wollen.\n",
    "\n",
    "Runden wir die Zahlen auf und ab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], grad_fn=<RoundBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(normalized_input_tensor).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...sieht man deutlich besser, dass wir die Gewichte so optimiert haben, dass die prediction für die Pixel-Werte einer 5 zu dem gleichen Output Vector führen wir unser Label, das wir definiert haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazit\n",
    "\n",
    "Was wir hier gemacht haben, war ein vereinfachtes Beispiel dafür, welche Rechenoperationen in einem neuronalen Netz stattfinden. Wir haben nur mit einem Bild \"trainiert\" und die Gewichte auf dieses Bild \"overfittet\" (das war sicherlich kein sinnvolles Training). Die Architektur dieses Netzes ist nicht unbedingt sinnvoll (- man muss solche Architekturen auch nicht unbedingt selbst erfinden können).\n",
    "\n",
    "ABER: Das was wir hier gesehen haben sind die Building Blocks von neuronalen Netzen. Das passiert unter der Haube. Matrix-Multiplikationen, Aktivierungsfunktionen, Ermittlung des Loss/Fehlers und Optimierungs-Schritte, um die Weights anzupassen - mehr nicht. Für moderne Deep Learning Verfahren und \"richtiges Training\" mit großen Datenmengen sind noch ein paar mehr Dinge nötig. Wir wollen ja nicht eine 5 \"auswändig lernen\" sondern generalisieren - trotzdem behalte dieses einfache Beispiel im Hinterkopf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
