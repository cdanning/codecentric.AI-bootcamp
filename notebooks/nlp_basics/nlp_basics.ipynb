{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# NLP Basics: Wer redet da?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "_Hallo und herzlich willkommen zum codecentric.ai bootcamp!_\n",
    "\n",
    "In diesem Tutorial beschäftigen wir uns mit Grundlagen der automatischen Textanalyse (NLP).\n",
    "\n",
    "Dazu haben wir einen Datensatz mit Reden deutscher Politiker ausgesucht und wollen ein Modell trainieren, das zu einer gegebenen Rede herausbekommt, wer sie gehalten hat! Trainiert wird das Modell mit Reden, zu denen es den Redner vorgesagt bekommt. Gemeinsam werden wir\n",
    "\n",
    "1. den Datensatz von der Quelle herunterladen, inspizieren und bereinigen,\n",
    "2. die Reden mit Hilfe der NLP-Bibliothek [spaCy](https://spacy.io) vorverarbeiten: tokenisieren, lemmatisieren und Begriffe extrahieren,\n",
    "3. statistische Informationen wie _bag of words_ extrahieren, um jede Rede durch einen Vektor darzustellen, der als Eingabe für ein Klassifikationsmodell genutzt werden kann,\n",
    "4. mit Hilfe der _machine learning_-Bibliothek [scikit-learn](https://sklearn.org) und der _deep learning_-Bibliothek [kerar](https://keras.io) verschiedene Modelle zur Klassifikation trainieren und\n",
    "5. abschließend als Vorbereitung für einen tieferen Einstieg Worteinbettungen betrachten und für die Klassifikation der Reden verwenden.\n",
    "\n",
    "Der folgende Screencast gibt einen kurzen Überblick über das Notebook.\n",
    "\n",
    "_Viel Spaß!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Datensatz einlesen, betrachten und bereinigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als Datensatz nutzen wir eine Sammlung von Reden, die Mitglieder der Bundesregierung beziehungsweise Bundespräsidenten gehalten haben, und die von Adrien Barbaresi zusammengestellt wurde: \n",
    "\n",
    "> \\[1\\] Barbaresi, Adrien (2018). _A corpus of German political speeches from the 21st century._ Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), European Language Resources Association (ELRA), pp. 792–797.\n",
    "\n",
    "Da diese Sammlung sehr umfangreich ist, haben wir im Verzeichnis `\"data\"` einen kleineren Auszug vorbereitet und \n",
    "\n",
    "- die Anzahl an Reden pro Redner auf 50 beschränkt,\n",
    "- die Reden ab dem 1000ten Zeichen abgeschnitten.\n",
    "\n",
    "Die folgende Funktion liest per default den reduzierten Auszug in einen [pandas](https://pandas.pydata.org)-DataFrame ein, lädt auf Wunsch aber auch den gesamten Datensatz herunter. Letzteres kann eine ganze Weile dauern und ist für dieses Tutorial nicht nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import zipfile\n",
    "import xmltodict\n",
    "\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DATA_FILE = \"speeches.json\"\n",
    "REMOTE_PATH = \"http://adrien.barbaresi.eu/corpora/speeches/\"\n",
    "REMOTE_FILE = \"German-political-speeches-2018-release.zip\"\n",
    "REMOTE_DATASET = \"Bundesregierung.xml\"\n",
    "REMOTE_URL = REMOTE_PATH + REMOTE_FILE\n",
    "\n",
    "\n",
    "def download_and_extract_data():\n",
    "    zip_path = os.path.join(DATA_PATH, REMOTE_FILE)\n",
    "    urllib.request.urlretrieve(REMOTE_URL, zip_path)\n",
    "    with zipfile.ZipFile(zip_path) as file:\n",
    "        file.extract(REMOTE_DATASET, path = DATA_PATH)\n",
    "\n",
    "\n",
    "def load_data(reduced=True):\n",
    "    if reduced:\n",
    "        return pd.read_json(os.path.join(DATA_PATH, DATA_FILE))\n",
    "    else:\n",
    "        zip_path = os.path.join(DATA_PATH, REMOTE_FILE)\n",
    "        if not os.path.isfile(zip_path):\n",
    "            download_and_extract_data()\n",
    "        file = os.path.join(DATA_PATH, REMOTE_DATASET) \n",
    "        with open(file, mode=\"rb\") as file:\n",
    "            xml_document = xmltodict.parse(file)\n",
    "            text_nodes = xml_document['collection']['text']\n",
    "            persons = [t['@person'] for t in text_nodes]\n",
    "            speeches = [t['rohtext']   for t in text_nodes]\n",
    "            return pd.DataFrame({'person' : persons, 'speech' : speeches})\n",
    "        \n",
    "        \n",
    "df = load_data(reduced=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Unser DataFrame `df` ist eine Tabelle mit zwei Spalten: `person` und `speech`. Jede Zeile enthält also in der ersten Spalte einen Redner und in der zweiten eine Rede, beides jeweils als Python-String. Schauen wir uns den DataFrame genauer an: die Methode\n",
    "- `head()` zeigt uns die ersten zehn Zeilen der Tabelle,\n",
    "- `value_counts()` zählt für jeden Redner, wie oft er in der Spalte auftaucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k.A.                       50\n",
       "Michael Naumann            50\n",
       "Bernd Neumann              50\n",
       "Gerhard Schröder           50\n",
       "Christina Weiss            50\n",
       "Monika Grütters            50\n",
       "Angela Merkel              50\n",
       "Julian Nida-Rümelin        47\n",
       "Thomas de Maizière         43\n",
       "Hans Martin Bury           42\n",
       "Joschka Fischer            31\n",
       "Rolf Schwanitz             24\n",
       "Frank-Walter Steinmeier     7\n",
       "Andere                      4\n",
       "Jullian Nida-Rümelin        1\n",
       "Name: person, dtype: int64"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.person.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir sehen, dass der Datensatz &mdash; wie so oft in der Praxis &mdash; Fehler beziehungsweise Unvollständigkeiten enthält:\n",
    "\n",
    "- Julian Nida-Rümelins Name wurde einmal falsch geschrieben;\n",
    "- für einige Reden wurde kein Redner angegeben, sondern \"k.A.\" oder \"Andere\". \n",
    "\n",
    "Wir korrigieren den Namen und entfernen alle Reden ohne Redner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df['person'] = df.person.str.replace(\"Jullian Nida-Rümelin\", \"Julian Nida-Rümelin\")\n",
    "df = df.query(\"person not in ['k.A.', 'Andere']\")\n",
    "df.person.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Werte können wir wie folgt visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_ = sns.countplot(y=\"person\", data=df) \\\n",
    "       .set(title=\"Anzahl der Reden\", xlabel=\"\", ylabel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## Analyse der Reden &mdash; von Daten zu Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "Die Reden liegen uns nun als Python-Strings vor. Zur Anwendung von machine learning oder deep learning müssen wir sie nun mit Hilfe von NLP weiterverarbeiten. Dafür gibt es zahlreiche Python-Bibliotheken wie\n",
    "\n",
    "- [NLTK](https://www.nltk.org/) (\"the natural language toolkit\"),\n",
    "- [spaCy](https://spacy.io/) (\"industrial strength natural language processing\"),\n",
    "- [gensim](https://radimrehurek.com/gensim/) (\"topic modelling for humans\")\n",
    "\n",
    "und viele andere. Wir verwenden in diesem Tutorial spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Automatische Textanalyse mit spaCy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als Erstes benötigt spaCy ein \"Modell\" der Sprache, mit der es arbeiten soll. Dieses Modell enthält statistische Informationen über die Sprache und wird für Deutsch von spaCy bereitgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Rufen wir  `nlp` mit einem Python-String auf, so wendet spaCy seine [Textanalyse-Pipeline](https://spacy.io/usage/processing-pipelines) darauf an und\n",
    "\n",
    "1. zerlegt den Text in _Token_ ([Tokenisierung](https://de.wikipedia.org/wiki/Tokenisierung)),\n",
    "2. bestimmt die Wortstämme beziehungsweise Grundformen der Token ([Lemmatisierung](https://de.wikipedia.org/wiki/Lemma_(Lexikographie)#Lemmatisierung)),\n",
    "3. bestimmt die Wortart ([part-of-speech tagging](https://de.wikipedia.org/wiki/Part-of-speech-Tagging),\n",
    "4. analysiert die grammatikalische Struktur der Text und\n",
    "5. extrahiert Begriffe ([named entity recognition](https://de.wikipedia.org/wiki/Part-of-speech-Tagging)).\n",
    "\n",
    "Schauen wir uns die Ergebnisse der Schritte 1-3 für einen Beispielsatz an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "document = nlp(\"Peter fährt in Berlin am schnellsten auf dem kaputten kunterbunten Fahrrad.\")\n",
    "pd.DataFrame({\"Token\": [word.text for word in document],\n",
    "              \"Grundform\": [word.lemma_ for word in document],\n",
    "              \"Wortart\": [word.pos_ for word in document]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir sehen, dass Schritt 2 nicht immer korrekte Ergebnisse liefert. Grund dafür ist z.B., dass das Modell nicht alle Wörter kennt. Außerdem verwendet spaCy selbst nicht eine Sammlung von Regeln, sondern neuronale Netze mit entsprechend unscharfen Ergebnissen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Anwendung auf die Reden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun wenden wir spaCy auf unsere Reden an und\n",
    "\n",
    "- extrahieren die jeweiligen Folgen von Token, Grundformen und Begriffen,\n",
    "- schalten dabei nicht benutzte Pipeline-Schritte wie die grammatikalische Analyse ab,\n",
    "- tragen die Ergebnisse in neue Spalten in unserem DataFrame 'df' ein.\n",
    "\n",
    "Weil das eine Weile dauert, speichern wir die Ergebnisse mittels [pickle](https://docs.python.org/3/library/pickle.html) ab und lesen sie das nächste Mal einfach wieder ein. Zeit für einen Kaffee?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ANALYSIS_FILE = \"speeches.pickle\"\n",
    "ANALYSIS_PATH = os.path.join(DATA_PATH, ANALYSIS_FILE)\n",
    "\n",
    "def analyze(speech):\n",
    "    with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "        document = nlp(speech)\n",
    "        token = [w.text for w in document]\n",
    "        lemma = [w.lemma_ for w in document]\n",
    "        entities = [e.text for e in document.ents]\n",
    "        return (token, lemma, entities)\n",
    "    \n",
    "if not os.path.isfile(ANALYSIS_PATH):\n",
    "    df[\"analysis\"] = df.speech.map(analyze)\n",
    "    df[\"tokens\"] = df.analysis.apply(lambda x: x[0])\n",
    "    df[\"lemmata\"] = df.analysis.apply(lambda x: x[1])\n",
    "    df[\"entities\"] = df.analysis.apply(lambda x: x[2])\n",
    "    df.to_pickle(ANALYSIS_PATH)\n",
    "else:\n",
    "    df = pd.read_pickle(ANALYSIS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Unser DataFrame sieht nun wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "###  Von Token zu Statistiken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die mit spaCy analysierten Reden verarbeiten wir jetzt weiter, indem wir\n",
    "\n",
    "1. das _Gesamt-Vokabular_ bestimmen, also die Menge aller Token, die in den Reden verwendet werden;\n",
    "2. das Gesamt-Vokabular durchnummerieren und ein Python-Dictionary `word2index` anlegen, das jedem Token seine Nummer zuordnet;\n",
    "3. für jede Rede  ermitteln, _welche_  Token darin auftauchen (die Menge ist das _bag of words_ der Rede) und _wie oft_ jedes Token darin auftaucht.\n",
    "\n",
    "Dasselbe machen wir auch für die _Grundformen_ der Token und die _extrahierten Begriffe_. Weitere interessante statistische Informationen wären etwa die _relative Häufigkeit_ oder das [tf-idf-Maß](https://de.wikipedia.org/wiki/Tf-idf-Ma%C3%9F) der Token.\n",
    "\n",
    "Dafür gibt es natürlich bereits fertige [Routinen](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text) in Bibliotheken wie [scikit-learn](https://sklearn.org). Aber erstens können wir diese Schritte einfach selbst bewältigen, und zweitens lernt man dabei mehr!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words(speeches):\n",
    "    word_sets = [set(speech) for speech in speeches]\n",
    "    all_words = set.union(*word_sets)\n",
    "    word2index = {word: index for (index, word) in enumerate(all_words)}\n",
    "    indexed_speeches = speeches.apply(lambda speech: [word2index[word]\n",
    "                                                      for word in speech])\n",
    "    return (indexed_speeches, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ein Beispiel macht klarer, was passiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "example = pd.DataFrame({\"Rede\": [\\\n",
    "    [\"Hallo\", \"Welt\", \"Du\", \"bist\", \"so\", \"schön\"], \\\n",
    "    [\"Hallo\", \"bist\", \"Du\", \"wach\", \"Du\"], \\\n",
    "    [\"die\", \"Welt\", \"ist\", \"groß\", \"ist\", \"die\", \"Welt\"]]}, index=[\"\",\"\",\"\"])\n",
    "\n",
    "example[\"Bag of Words\"], example_index =  bag_of_words(example.Rede)\n",
    "pd.DataFrame(example_index, index=[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als zweite Variante zählen wir für jedes Token einer Rede, wie oft es in der Rede auftritt &mdash; daher die Bezeichnung `count`. Dafür ist die Klasse [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) hilfreich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_words(speeches, word2index):\n",
    "    def count_speech(speech):\n",
    "        return {word2index[word]: count\n",
    "                for (word, count) in Counter(speech).items()}\n",
    "    return speeches.apply(count_speech)\n",
    "\n",
    "example[\"Counts\"] = count_words(example[\"Rede\"], example_index)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Für eine einheitliche Weiterverarbeitung wandeln wir die bags-of-words ebenfalls in Dictionarys um, die jedem Token einfach eine 1 zuordnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bags_of_words_to_dicts(bags_of_words):\n",
    "    return bags_of_words.apply(lambda bow: {word: 1 for word in bow})\n",
    "\n",
    "example[\"Bag of Words as dict\"] = bags_of_words_to_dicts(example[\"Bag of Words\"])\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir wenden all das nun auf unsere Reden an und verwenden dabei nicht nur die Token, sondern auch deren Grundformen und die extrahierten Begriffe. Die Ergebnisse tragen wir wieder in unseren DataFrame `df` in neue Spalten ein. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "PARTS = [\"tokens\", \"lemmata\", \"entities\"]\n",
    "INDEX = dict()\n",
    "\n",
    "for part in PARTS:\n",
    "    (bow, INDEX[part]) = bag_of_words(df[part])\n",
    "    df[part + \"_bow\"] = bags_of_words_to_dicts(bow)\n",
    "    df[part + \"_count\"] = count_words(df[part], INDEX[part])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Exemplarisch betrachten wir einige der extrahierten Daten der ersten Rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## Klassifikation der Reden mit machine learning und scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "Wir trainieren nun ein Modell darauf, zu einer gegebenen Rede den jeweiligen Redner zu bestimmen! Dazu verwenden wir\n",
    "\n",
    "- als \"Fingerabdruck\" jeder Rede die extrahierten statistischen Informationen und\n",
    "- als Modell zuerst ein klassisches _machine learning_-Verfahren, einen [Bayes-Klassifikator](https://de.wikipedia.org/wiki/Bayes-Klassifikator),\n",
    "- die Standard-machine learning-Bibliothek [scikit-learn](https://sklearn.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Obwohl wir die Daten bereits aufbereitet haben, sind für das Training und das Testen des Modells noch ein paar Vorbereitungen erforderlich.\n",
    "\n",
    "Um zu sehen, welchen Einfluss die Datenmenge und -auswahl auf die Genauigkeit der Klassifikation hat, wählen wir zunächst mit `sample_speeches` pro Redner eine feste Anzahl `num_samples`von Reden zufällig aus dem Datensatz `data` aus. Sind nicht genügend Reden vorhanden, wird der Redner herausgefiltert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "NO_SAMPLES = 0\n",
    "\n",
    "def sample_speeches(data, num_samples):\n",
    "    # lokale Hilfsfunktion, führt Sampling und Entfernung von Personen aus\n",
    "    def sample(group):\n",
    "        if len(group) >= num_samples:\n",
    "            return group.sample(num_samples)\n",
    "        else:\n",
    "            return group.sample(NO_SAMPLES)\n",
    "    return data.groupby(\"person\", group_keys=False).apply(sample).reset_index(drop=True)\n",
    "\n",
    "# beispielhaftes Sampling\n",
    "sampled_df = sample_speeches(df, 20)\n",
    "sampled_df[\"person\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als nächstes zerlegen wir mit `split` die aufbereiteten Daten `data` &mdash; die Fingerabdrücke der Reden &mdash; und die zugehörigen `label` &mdash; die Liste der jeweiligen Redner &mdash; im Verhältnis `ratio` zu 1-`ratio` in ein Trainings-Daten und Test-Daten. Dabei erfolgt die Auswahl zufällig und die Daten werden permutiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def split(data, labels, ratio):\n",
    "    size = data.shape[0]\n",
    "    split_index = int(ratio * size)\n",
    "    indices = np.random.permutation(size)\n",
    "    train_indices, test_indices = (indices[:split_index], indices[split_index:])\n",
    "    return (data[train_indices], labels[train_indices],\n",
    "            data[test_indices], labels[test_indices])\n",
    "\n",
    "# beispielhaftes Zerlegen, sodass zwei Trainingsdatenpunkte entstehen\n",
    "train_word, train_label, test_word, test_label = split(pd.Series([\"Das\", \"ist\", \"ein\", \"Test\"]), \n",
    "                                                       pd.Series([1, 2, 3, 4]), 0.6)\n",
    "train_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Für jede Rede haben wir bisher die statistischen Informationen in Python-Dictionarys abgelegt. Der Bayes-Klassifikator benötigt diese Daten aber in Form eines zweidimensionalen Arrays, bei dem jede Zeile einer Rede entspricht, jede Spalte einem Token (bzw. Grundform oder Begriff) des Gesamtvokabulars. Da das Gesamtvokabular sehr groß ist, in jeder Rede aber davon nur ein sehr kleiner Bruchteil verwendet wird, würde dieses Array hauptsächlich Nullen enthalten. Das Modul\n",
    "[scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) bietet für solche [dünn besetzten](https://de.wikipedia.org/wiki/D%C3%BCnnbesetzte_Matrix) Arrays speziell optimierte Datenstrukturen.\n",
    "Mit `dict_to_sparse` wandeln wir nun eine Liste der Python-Dictionarys in eine dünn besetzte Matrix um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import itertools\n",
    "\n",
    "def dict_to_sparse(list_of_dicts, vocab_size):\n",
    "    index_and_data = [(index, dictionary[index])  for dictionary in list_of_dicts \n",
    "                          for index in dictionary]\n",
    "    index, data = zip(*index_and_data)\n",
    "    lens = [len(dictionary) for dictionary in list_of_dicts]\n",
    "    row_pointers = list(itertools.accumulate([0] + lens))\n",
    "    return sparse.csr_matrix((data, index, row_pointers), \n",
    "                             shape=(len(list_of_dicts), vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dict_to_sparse(example[\"Bag of Words as dict\"], len(example_index)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Training und Test des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Training und Test des Bayes-Klassifikators kapseln wir in eine separate Routine `train_and_test`. Neben den Daten und Labels erwartet diese als Parameter auch das zu wählende Verhältnis zwischen Trainingsdaten und Testdaten.\n",
    "Die Methode `fit` führt das eigentliche Training durch und die Methode `predict` den Test. Die Genauigkeit des Modells messen wir wie folgt:\n",
    "\n",
    "- `accuracy` gibt an, welcher Anteil der Vorhersagen korrekt war, ist also ein einfaches Maß für die Genauigkeit;\n",
    "- `confusion` ist eine Matrix, deren Zeilen und Spalten mit den vorhandenen Rednern durchnummeriert sind und deren i-te Zeile und j-te Spalte angibt, wieviel Reden des i-ten Redners von dem Modell für eine Rede des j-ten Redners gehalten wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train_and_test(data, labels, split_ratio):\n",
    "    train_data, train_labels, test_data, test_labels = split(data, labels,\n",
    "                                                             split_ratio)\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(train_data, train_labels)\n",
    "    predictions = classifier.predict(test_data)\n",
    "    accuracy = np.mean(predictions == test_labels)\n",
    "    confusion = confusion_matrix(test_labels, predictions)\n",
    "    return (accuracy, confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Das Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun kommen wir zum eigentlichen Experiment! Wir trainieren und testen nacheinander und unabhängig voneinander einen Bayes-Klassifikator mit den extrahierten statistischen Informationen und machen für jede Kombination von\n",
    "Features und Statistiken mehrere Durchläufe. Anschließend visualisieren wir die Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "FEATURES = [\"tokens\", \"lemmata\", \"entities\"]\n",
    "STATS = [\"bow\", \"count\"]\n",
    "REPETITIONS = 60\n",
    "SPLIT_RATIO = 0.7\n",
    "NUM_SAMPLES = 50\n",
    "\n",
    "def bayes_experiment(features=FEATURES, stats=STATS, repetitions=REPETITIONS,\n",
    "                     num_samples=NUM_SAMPLES, split_ratio=SPLIT_RATIO):\n",
    "    sampled_df = sample_speeches(df, num_samples)\n",
    "    labels = sampled_df[\"person\"].astype(\"category\").cat.codes\n",
    "    print(\"Anzahl der Redner: \", np.max(labels)+1)\n",
    "    results = []\n",
    "    for feature in features:\n",
    "        for stat in stats:\n",
    "            data = dict_to_sparse(sampled_df[feature + \"_\" + stat],\n",
    "                                  len(INDEX[feature]))\n",
    "            print(\"Klassifizierung anhand Statistik '%s' für Feature '%s'\" \n",
    "                  % (stat, feature))\n",
    "            for i in range(0, repetitions):\n",
    "                (accuracy, confusion) = train_and_test(data, labels, split_ratio)\n",
    "                results.append({\"feature\": feature, \n",
    "                                \"stat\": stat, \n",
    "                                \"repetition\": i,\n",
    "                                \"accuracy\": accuracy, \n",
    "                                \"confusion_matrix\": confusion})\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir führen das Experiment zuerst mit 25 Reden pro Person durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results_few_samples = bayes_experiment(num_samples = 25)\n",
    "results_few_samples.groupby([\"feature\", \"stat\"])[\"accuracy\"].mean().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die erzielte Genauigkeit der Vorhersagen ist recht gering &mdash; durch zufälliges Raten würden wir bei 10 Rednern im Schnitt eine Genauigkeit von 0.1 beziehungsweise 10% erzielen.\n",
    "\n",
    "Folgender Plot zeigt, dass die Genauigkeit außerdem stark von der zufälligen Auswahl der Trainings- und Testdaten abhängt, aber kaum davon, welche Features oder Statistik wir verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(data, title, kind=\"swarm\"):\n",
    "    sns.catplot(data=data, col=\"feature\", x=\"stat\", y=\"accuracy\", kind=kind) \\\n",
    "        .set(xlabel=\"verwendete Satistik\", ylabel=\"Genauigkeit\")\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "plot_accuracies(results_few_samples, \"Bei wenig Daten schwankt die Genauigkeit der Klassifikation stark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als nächstes plotten wir heatmaps für die  [Konfusionsmatrizen](https://en.wikipedia.org/wiki/Confusion_matrix). Wie bereits erklärt geben diese an, wieviel Reden richtig (Diagonaleinträge) beziehungsweise falsch (die anderen Einträge) zugeordnet wurden. Wir gruppieren diese Matrizen nach Feature und Statistik und summieren über die Durchläufe auf.\n",
    "\n",
    "So zeigt sich, dass zum Beispiel für die Features Lemmata und Tokens (die beiden rechten Spalten) ein bis zwei Redner bevorzugt vorausgesagt wurden (\"helle\" Spalten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(results):\n",
    "    def plot_heatmap(data, **kwargs):\n",
    "        sns.heatmap(data[\"confusion_matrix\"].iloc[0])\n",
    "    confusion_matrix = results.groupby([\"feature\", \"stat\"])[\"confusion_matrix\"] \\\n",
    "                              .apply(lambda cms: np.sum(cms, axis=0)) \\\n",
    "                              .reset_index()   \n",
    "    g = sns.FacetGrid(confusion_matrix, col=\"feature\", row=\"stat\")\n",
    "    g.map_dataframe(plot_heatmap)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.suptitle(\"Summierte 'Confusion'-Matrizen der Klassifikation\")\n",
    "\n",
    "plot_confusion_matrices(results_few_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun führen wir dasselbe Experiment noch einmal mit 50 Reden pro Person durch und schauen, wie sich die Vorhersagegenauigkeit verbessert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results_many_samples = bayes_experiment(num_samples=50)\n",
    "results_many_samples.groupby([\"feature\", \"stat\"])[\"accuracy\"].mean().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Voraussagekraft steigt wie zu erwarten an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_accuracies(results_many_samples, \"Mehr Daten heben die Genauigkeit und senken die Streuung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Diagramme sehen nett aus, aber die durchschnittlich erzielte Genauigkeit lässt sich daran beispielsweise nicht gut ablesen. Dafür bieten sich boxplots an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_accuracies(results_many_samples, \"Mehr Daten heben die Genauigkeit und senken die Streuung\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## Klassifikation mit einem neuronalen Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun tauschen wir den Bayes-Klassifikator gegen ein neuronales Netz aus. Wir nutzen dazu die deep learning-Bibliothek [keras](https://keras.io) und wählen ein einfache Netz-Architektur: mehrere sequentiell aufeinander folgenden Schichten von Neuronen. Die dafür benötigte Funktionalität stellt `keras` mit der Modell-Klasse `Sequential` und dem Untermodul `layers`bereit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Die Architektur des neuronalen Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als *Eingabe* für unser Netz verwenden wir dieselben Daten wie zuvor: jede Rede wird durch einen Vektor dargestellt, dessen i-te Komponente eine statistische Größe für das i-te Token des Gesamtvokabulars enthält. Dieser Eingabevektor wird von einer *ersten Schicht von Neuronen* aufgegriffen. Jedes Neuron dieser Schicht \n",
    "  * wichtet die Komponenten des Eingabevektors mit Gewichten, die das Neuron in der Trainingsphase optimiert,\n",
    "  * bildet anschließend die Summe der gewichteten Komponeten und \n",
    "  * wendet auf die Summe eine _Aktivierungsfunktion_ an, hier [`relu`](https://de.wikipedia.org/wiki/Rectifier_(neuronale_Netzwerke)). \n",
    "\n",
    "Die Werte dieser Aktivierungsfunktionen bilden den Ausgabevektor der ersten Schicht. Die Ausgabe der ersten Schicht dient als Eingabe für *eine zweite Schicht* von Neuronen, die nach demselben Prinzip ihrerseits einen Ausgabevektor bildet. Solche dicht vernetzten Schichten können wir nun munter aufeinanderstapeln.\n",
    "\n",
    "Als *Ausgabe* soll uns das Netz sagen, von welchem Politiker die Rede stammt. Dafür  verwenden wir eine Schicht mit genau soviel Neuronen, wie Politiker auftreten: das i-te Neuron gibt uns die vom Netz vermutete Wahrscheinlichkeit dafür an, dass die Rede von dem i-ten Politiker stammt. Diese Wahrscheinlichkeiten müssen sich zu 1 aufsummieren. Das sichern wir mit Hilfe der Aktivierungsfunktion [softmax](https://de.wikipedia.org/wiki/Softmax-Funktion).\n",
    "\n",
    "Den Aufbau des Netzes übernimmt die folgende Funktion `build_seq_model`. Ihre Parameter sind\n",
    "- `x_dim` - die Dimension des Eingabevektors (Größe des Vokabulars),\n",
    "- `y_dim` - die Dimension des Ausgabevektors (Anzahl der Politiker) und\n",
    "- `architecture` - die Anzahl der Neuronen von der ersten bis zur vorletzten Schicht (beziehungsweise die [Dropout-Rate](https://de.wikipedia.org/wiki/Dropout_(k%C3%BCnstliches_neuronales_Netz)), die erst später relevant wird).\n",
    "\n",
    "Bevor wir das Netz trainieren können, muss es noch kompiliert werden. Dazu ist eine Loss-Funktion anzugeben, welche Keras während des Trainings mit dem angegebenen Optimierer zu minimieren versucht. Da die Ausgabe des Netzes eine Wahrscheinlichkeitsverteilung für die gegebenen Politiker ist, wählen wir hier die [Kreuzentropie](https://de.wikipedia.org/wiki/Kreuzentropie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def build_seq_model(x_dim, y_dim, architecture):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(architecture[0], input_shape=(x_dim,),\n",
    "                           activation='relu'))\n",
    "    for s in architecture[1:]:\n",
    "        if s >= 1:\n",
    "            model.add(layers.Dense(s, activation='relu'))\n",
    "        else:\n",
    "            # dieser Fall wird erst später genutzt, erstmal ignorieren!\n",
    "            model.add(layers.Dropout(rate=s))\n",
    "    model.add(layers.Dense(y_dim, activation=\"softmax\"))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Training und Test des Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Bevor wir das Netz trainieren können, müssen wir uns noch über die Labels Gedanken machen: Während der Bayes-Klassifikator erwartet hat, dass wir die Politiker durchnummerieren, benötigt unser neuronales Netz für Label dasselbe Format, was es auch ausgibt: Wahrscheinlichkeitsverteilungen. Genauer gesagt müssen wir unsere Politiker [one-hot-encoden](https://de.wikipedia.org/wiki/1-aus-n-Code): ist die Rede vom i-ten Politiker, so ist das zugehörige Label der Vektor, der an der i-ten Stelle eine 1 und für alle anderen Politiker eine 0 enthält. Das geht mit folgendem `numpy`-Trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    dim = np.max(y) + 1\n",
    "    return np.eye(dim)[y]\n",
    "\n",
    "one_hot_encode([0,8,1,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun können wir das Netz trainieren. Dazu verwenden wir das Gegenstück zu der obigen Funktion `train_and_test`.\n",
    "Weil das Training ein ganzes Stück länger dauert als beim Bayes-Klassifikator, konzentrieren wir uns aber auf ein Feature (\"token\") und eine Statistik (\"bof\").\n",
    "\n",
    "Erst werden die Daten und Labels für das Training und den Test gesampelt und aufgeteilt, dann die Labels wie beschrieben kodiert und anschließend das Netz gebaut und trainiert. Die Testdaten und -labels werden hier schon übergeben - damit können wir später den Lernprozess besser auswerten. \n",
    "Die Parameter `epochs` und `batch_size` steuern die Dauer und Granularität des Lernprozesses. \n",
    "\n",
    "Das Training liefert ein `History`-Objekt mit Informationen über den Verlauf des Trainings zurück, mehr dazu gleich. \n",
    "Die vom Modell getroffenen Vorhersagen sind Wahrscheinlichkeitsverteilungen. Mit der numpy-Funktion `argmax` bestimmen wir für solch eine Wahrscheinlichkeitsverteilung den Index mit dem größten Wert - das ist dann der von unserem Netz vermutetete Redner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.7\n",
    "EPOCHS = 10\n",
    "FEATURE = \"tokens\"\n",
    "STAT = \"bow\"\n",
    "\n",
    "def train_and_test_net(model, data, labels, epochs): \n",
    "    train_data, train_labels, test_data, test_labels = split(data, labels,\n",
    "                                                             SPLIT_RATIO)\n",
    "    train_labels_ohe = one_hot_encode(train_labels)\n",
    "    test_labels_ohe = one_hot_encode(test_labels)\n",
    "    history = model.fit(train_data, train_labels_ohe,\n",
    "                        validation_data=(test_data, test_labels_ohe),\n",
    "                        epochs=epochs)\n",
    "    prediction_probas = model.predict(test_data)\n",
    "    predictions = np.argmax(prediction_probas, axis=1)\n",
    "    accuracy = np.mean(predictions == test_labels)\n",
    "    confusion = confusion_matrix(test_labels, predictions)\n",
    "    return (accuracy, confusion, history)\n",
    "\n",
    "def train_and_test_dense_net(num_samples, architecture, epochs=EPOCHS):\n",
    "    sampled_df = sample_speeches(df, num_samples)\n",
    "    labels = sampled_df[\"person\"].astype(\"category\").cat.codes\n",
    "    data = dict_to_sparse(sampled_df[FEATURE + \"_\" + STAT], len(INDEX[FEATURE]))\n",
    "    model = build_seq_model(data.shape[1], np.max(labels) + 1, architecture)\n",
    "    return train_and_test_net(model, data, labels, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun sind wir bereit für den ersten Testlauf!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ARCHITECTURE = [256, 32]\n",
    "\n",
    "(accuracy, confusion, history) = train_and_test_dense_net(50, ARCHITECTURE)\n",
    "print(\"Genauigkeit: \", accuracy)\n",
    "_ = sns.heatmap(confusion).set(title=\"'Confusion'-Matrix für die Klassifikation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das zusätzliche History-Objekt enthält Informationen über den Verlauf des Trainings, die wir wie folgt visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    loss_dF = pd.DataFrame({\"Trainings-Loss\": history.history[\"loss\"],\n",
    "                            \"Test-Loss\": history.history[\"val_loss\"]})\n",
    "    loss_dF.plot.line().set(xlabel=\"Epoche\", ylabel=\"Loss\", title=\"Trainingsverlauf\")\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das Bild zeigt den Verlauf der Loss-Funktion nach jeder Epoche des Trainings. Genauer sehen wir\n",
    "die Kreuzentropie zwischen den Vorhersagen und den tatsächlichen one-hot-kodierten Labels\n",
    "- für die Trainingsdaten (`loss`) und\n",
    "- für die Testdaten (`val_loss`).\n",
    "\n",
    "Wir sehen, dass die Loss-Funktion  sehr schnell erfolgreich minimiert wurde - aber nur auf den Trainingsdaten! Das Netz kann das Gelernte offenbar schlecht auf die Testdaten übertragen. Sinkt die Genauigkeit der Vorhersage mit fortschreitendem Training sogar wieder, so spricht man von [Überanpassung/Overfitting](https://de.wikipedia.org/wiki/%C3%9Cberanpassung). Diese tritt stets dann ein, wenn man zu wenig oder nicht repräsentative Trainingsdaten hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Mit Dropout die Genauigkeit verbessern &mdash; das Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wie können wir die Leistung des Modells verbessern? Ein gutes Mittel lautet [Dropout](https://de.wikipedia.org/wiki/Dropout_(k%C3%BCnstliches_neuronales_Netz)) &mdash; bei jedem Lernschritt wird ein Teil der Neuronen \"ausgeblendet\". In `keras` erreicht man dies durch zusätzliche _Dropout-Schichten_. Diese werden zwischen die gewöhnlichen Schichten eingefügt und erhalten als Parameter die _Dropout-Rate_, die angibt, welcher Prozentsatz der Neuronen in der vorherigen Schicht \"ausgeblendet\" werden sollen. Unsere Routine `build_seq_model` fügt immer dann eine Dropout-Schicht ein, wenn der Größen-Parameter für die aktuelle Schicht kleiner 1 ist.\n",
    "\n",
    "Ob dies aber wirklich die Ergebnisse verbessert, können wir nicht an einem Test-Durchlauf ablesen: wie beim Bayes-Klassifikator wird auch hier die Genauigkeit stark von der zufälligen Auswahl der Test- und Trainingsdaten abhängen. Deswegen führen wir ähnlich wie zuvor ein systematisches Experiment durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def neural_experiment(train_and_test_fn, num_samples, rounds, architectures, epochs):\n",
    "    losses = dict()\n",
    "    accuracies = dict()\n",
    "    for architecture in architectures:\n",
    "        print(\"Testdurchlauf für Netzarchitektur \", architecture)\n",
    "        for rnd in range(rounds):\n",
    "            print(\"Runde \", rnd + 1, \"/\", rounds)\n",
    "            (accuracy, _, history) = train_and_test_fn(num_samples, \n",
    "                                                       architecture=architecture, \n",
    "                                                       epochs=epochs)\n",
    "            losses[(str(architecture), \"Trainings-Loss\", rnd)] = history.history[\"loss\"]\n",
    "            losses[(str(architecture), \"Test-Loss\", rnd)] = history.history[\"val_loss\"]\n",
    "            accuracies[(str(architecture), rnd)] = accuracy\n",
    "    return (pd.DataFrame(accuracies, index=[0]),\n",
    "            pd.DataFrame(losses, index=range(0, epochs)).rename_axis(\"Epoche\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das folgende Experiment kann je nach Rechner eine Weile dauern. Zeit für einen Kaffee! Oder einfach das Experiment erstmal überspringen &mdash; wir haben es dafür auskommentiert, einmal bereits durchgeführt und die Ergebnisse abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_PATH = \"experiments\"\n",
    "\n",
    "ARCHITECTURES = [[256, 64, 16], [256, 64, 0.5, 16],\n",
    "                 [1024, 64], [1024, 0.5, 64]]\n",
    "\n",
    "def save_results(accuracies, losses, name): \n",
    "    accuracies.to_pickle(os.path.join(EXPERIMENT_PATH, name + \"_accuracies\"))\n",
    "    losses.to_pickle(os.path.join(EXPERIMENT_PATH, name + \"_losses\"))\n",
    "   \n",
    "\n",
    "#(accuracies, losses) = neural_experiment(train_and_test_dense_net, \n",
    "#                                         num_samples=50, rounds=10, \n",
    "#                                         architectures=ARCHITECTURES, epochs=10)\n",
    "#save_results(accuracies, losses, \"dense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir lesen nun die gerade berechneten oder vorher schon abgespeicherten Ergebnisse wieder ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(name):\n",
    "    accuracies = pd.read_pickle(os.path.join(EXPERIMENT_PATH, name + \"_accuracies\"))\n",
    "    losses = pd.read_pickle(os.path.join(EXPERIMENT_PATH, name + \"_losses\"))\n",
    "    return (accuracies, losses)\n",
    "\n",
    "accuracies, losses = load_results(\"dense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die folgende Grafik zeigt, wie für jede Netzwerk-Architektur die Genauigkeit in den jeweiligen Durchläufen streut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_net_accuracies(accuracies):\n",
    "    acc_long = accuracies.melt(var_name=[\"Netzarchitektur\", \"Runde\"], value_name=\"Genauigkeit\")\n",
    "    sns.boxplot(y=\"Netzarchitektur\", x=\"Genauigkeit\", data=acc_long)\n",
    "    \n",
    "plot_net_accuracies(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hier zeigt sich, dass Dropout die Ergebnisse jeseil deutlich verbessert.\n",
    "\n",
    "Zum Schluss betrachten wir noch, wie sich Trainings-Loss und Validierungs-Loss während des Lernprozesses entwickelt haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    loss_long = losses.reset_index().melt(id_vars=\"Epoche\", \n",
    "                                          var_name=[\"Netzarchitektur\", \"\",\n",
    "                                                    \"Runde\"],\n",
    "                                          value_name=\"Loss\")\n",
    "    sns.relplot(x=\"Epoche\", hue=\"\", col=\"Netzarchitektur\", col_wrap=2, \n",
    "                y=\"Loss\", data=loss_long, kind=\"line\")\n",
    "    \n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## Wort-Einbettungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "### Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "Bisher haben wir für die Klassifikation nur statistische Informationen über die Token, Wortstämme beziehungsweise named entitites genutzt. Wo beziehungsweise in welcher Reihenfolge diese in der Rede auftraten, spielte keine Rolle. Durch unsere statistische Brille betrachtet könnten wir also die beiden Sätze\n",
    "    \n",
    "    \"Ich bin Gargamel und hasse Papa Schlumpf.\"\n",
    "    \n",
    "und\n",
    "\n",
    "    \"Ich bin Papa Schlumpf und hasse Gargamel.\"\n",
    "    \n",
    "nicht unterscheiden! Für manche Anwendungen von NLP ist das in Ordnung, aber für manche muss man den Text als eine _Folge_ von Token behandeln. \n",
    "Dabei müssen wir die Token, die bisher Zeichenketten sind, durch Zahlen beziehungsweise Vektoren darstellen. Die Frage ist, wie?\n",
    "\n",
    "1. Ein erster Ansatz wäre: _einfach durchnummerieren_. Das Problem dabei: (fast) alle Algorithmen würden Token mit nahe beieinander liegenden Nummern als ähnlich ansehen.\n",
    "2. Eine bessere Idee: _one-hot-kodieren_, wie wir das bereits mit den Labeln gemacht haben. \n",
    "Das Problem hierbei: für jedes Token erhalten wir einen Vektor mit hunderttausenden oder mehr Komponenten  - je nachdem, wie groß unser Vokabular ist. Bei solchen Eingaben versagen alle bekannten Algorithmen (siehe [Fluch der hohen Dimensionen](https://de.wikipedia.org/wiki/Fluch_der_Dimensionalit%C3%A4t)).\n",
    "3. Die übliche Lösung: wir verwenden eine _spezielle dichte Einbettung_, englisch _word embedding_, die jedem Wort einen Vektor zuordnet, der mehrere Hundert Komponenten hat, aber nicht hunderttausend oder mehr wie im Ansatz 2.\n",
    "\n",
    "Doch woher kann man solch eine dichte Einbettung nehmen? Die wichtigsten Algorithmen, die solche Einbettungen aus einem riesigen Textvorrat (wie etwa Wikipedia-Seiten) berechnen, sind [word2vec](https://en.wikipedia.org/wiki/Word2vec) und [gloVe](https://nlp.stanford.edu/projects/glove/). Für die englische Sprache ist solch eine Einbettung in [spacy](https://www.spacy.io) verfügbar, für Deutsch und sehr viele weitere Sprachen zum Beispiel in [fasttext](https://fasttext.cc). Wir greifen hier auf eine [Wort-Einbettung](https://devmount.github.io/GermanWordEmbeddings/) von [Andreas Müller](https://github.com/devmount) zurück, die er mit Hilfe der NLP-Bibliothek [gensim](https://radimrehurek.com/gensim/) erstellt hat. Da die Datei der Wort-Vektoren _sehr_ groß ist (704 MB), haben wir eine Teilmenge für die von uns benötigten Worten extrahiert und bereits abgespeichert. Wer möchte, kann die gesamte Originaldatei herunterladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import gensim\n",
    "import gzip\n",
    "\n",
    "WV_FILE = \"german.model\"\n",
    "WV_PATH = os.path.join(DATA_PATH, WV_FILE)\n",
    "WV_DIM = 300\n",
    "\n",
    "\n",
    "def load_embedding(reduced=True): \n",
    "    if reduced:\n",
    "        return gensim.models.KeyedVectors.load(WV_PATH + \".reduced\")\n",
    "    else:\n",
    "        if not os.path.isfile(WV_PATH):\n",
    "            WV_URL = \"http://cloud.devmount.de/d2bc5672c523b086/german.model\"\n",
    "            urllib.request.urlretrieve(WV_URL, WV_PATH)\n",
    "        return gensim.models.KeyedVectors.load_word2vec_format(WV_PATH, binary=True)\n",
    "            \n",
    "w2v = load_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Was ist an Wort-Einbettungen so spannend und verblüffend? Sowohl der Abstand als auch die Addition und Subtraktion von Vektoren sind erstaunlich \"sinnvoll\". So kann man mit Wort-Einbettungen Tabu spielen! _Achtung:_ für `w2v` müssen die Buchstaben `ä`, `ö`, `ü`, `ß` durch `ae`, `oe`, `ue`, `ss` ersetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutschland + Kontinent => Europa (0.876782)\n",
      "Jahreszeit + Schnee + Sommer => Winter (0.815416)\n",
      "Stuhl + Gemuetlich => Sofa (0.780442)\n",
      "Fest + Winter + Christkind => Eiersuchen (0.754445)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timmermann/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "TABU_AUFGABEN = [[\"Deutschland\", \"Kontinent\"],\n",
    "                 [\"Jahreszeit\", \"Schnee\", \"Sommer\"],\n",
    "                 [\"Stuhl\", \"Gemuetlich\"],\n",
    "                 [\"Fest\", \"Winter\", \"Christkind\"],\n",
    "                ]\n",
    "\n",
    "def tabu(aufgaben=TABU_AUFGABEN):\n",
    "    for woerter in aufgaben: \n",
    "        antwort, sicherheit = w2v.most_similar(woerter, topn=1)[0]\n",
    "        print(\" + \".join(woerter), \"=>\",  antwort, \"(%f)\" % sicherheit)\n",
    "        \n",
    "tabu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Antworten sind gar nicht so schlecht, oder?\n",
    "\n",
    "Interessant ist auch, sich eine Projektion mehrerer Wortvektoren in der Ebene anzuschauen. Eine geeignete Projektionsebene wählen wir mit Hilfe einer [Hauptkomponentenanalyse/principal component analysis](https://de.wikipedia.org/wiki/Hauptkomponentenanalyse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "WORTLISTE = [\"Mutter\", \"Tochter\", \"Sohn\", \"Opa\", \"Oma\", \n",
    "             \"Vater\", \"Grossvater\", \"Grossmutter\"]\n",
    "\n",
    "def plot_word_vectors(woerter=WORTLISTE):\n",
    "    pca = PCA(n_components=2)\n",
    "    coords = pca.fit_transform(np.vstack([w2v[w] for w in woerter]))\n",
    "    plt.axis((-1.5,2.5,-1.5,1.5))\n",
    "    plt.scatter(coords[:,0], coords[:,1])\n",
    "    for (i, w) in enumerate(woerter):\n",
    "        plt.annotate(w, xy=coords[i] + (0.05,0.05))\n",
    "    plt.title(\"Wortvektoren kodieren Semantik\")\n",
    "        \n",
    "plot_word_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun wollen wir schließlich noch eine Klassifikation mit Hilfe der Worteinbettungen trainieren. \n",
    "\n",
    "Dazu nehmen wir von jeder Rede die ersten `SAMPLE_LEN` Token, schreiben deren jeweilige Wortvektoren nebeneinander in ein 2-dimensionales numpy-Array und verwenden dieses Array als Darstellung der Rede für die Klassifikation. Dabei müssen wir bei den Token Umlaute ersetzen und die Wörter groß schreiben. Zur Kontrolle notieren wir auch, für welche Token `w2v` _keinen_ Wortvektor enthält. Doch bevor wir anfangen, werfen wir erstmal einen Blick darauf, wieviel Token unsere abgeschnittenen Reden eigentlich enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens_len\"] = df.tokens.map(len)\n",
    "_ = sns.boxenplot(x=\"tokens_len\", y=\"person\", data=df).set(title=\"Länge der abgeschnittenen Reden in Token\", xlabel=\"\", ylabel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir begrenzen die Redenlängen nun auf 150 Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_LEN = 150\n",
    "WV_DIM = 300\n",
    "\n",
    "def replace(text):\n",
    "    pattern = [('ä', 'ae'), ('ö', 'oe'), ('ü', 'ue'), ('Ä', 'Ae'), ('Ö', 'Oe'), ('Ü', 'Ue'), ('ß', 'ss')]\n",
    "    for (old, new) in pattern: \n",
    "        text = text.replace(old, new)\n",
    "    return text[0].upper() + text[1:]\n",
    "\n",
    "\n",
    "def wvseq(speech, sample_len=SAMPLE_LEN):\n",
    "    vectors = np.zeros((sample_len, WV_DIM))\n",
    "    (j, t, found, missed) = (0, 0, [], []) \n",
    "    for (j,token) in zip(range(0,sample_len), speech):\n",
    "        tok = replace(token)\n",
    "        if tok in w2v:\n",
    "            vectors[j] = w2v[tok]\n",
    "            found.append(tok)\n",
    "        else:\n",
    "            missed.append(tok)\n",
    "    return (vectors, found, missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "speech = [\"Hallo\", \"das\", \"ist\", \"interessanterweise\", \"XYZ\", \"unklar\", \"!\"]\n",
    "(vectors, found, missed) = wvseq(speech, len(speech))\n",
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun wenden wir diese Vorverarbeitung auf unsere Reden an und prüfen, welcher Prozentsatz an Token in `w2v` fehlt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "wvs = df.lemmata.map(wvseq)\n",
    "df[\"word_vectors\"] = wvs.map(lambda x: x[0])\n",
    "df[\"missed_prop\"] = wvs.map(lambda x: len(x[2])/(len(x[1])+len(x[2])))\n",
    "_ = sns.boxplot(df.missed_prop).set(xlabel=\"\", title=\"Anteil fehlender Token in den Reden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden nun die vorverarbeiteten Daten zur Klassifikation mit Hilfe eines neuronalen Netzwerks, das im Wesentlichen aus Faltungs-, Batchnormalisierungs- und Poolings-Schichten bestent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "def build_conv_model(architecture, y_dim):\n",
    "    model = Sequential()\n",
    "    (c1, k1, p1, m1, c2, k2, p2, m2) = architecture\n",
    "    model.add(layers.Conv1D(c1, k1, input_shape=(SAMPLE_LEN, WV_DIM)))\n",
    "    model.add(layers.BatchNormalization(momentum=m1))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.AveragePooling1D(p1))\n",
    "    model.add(layers.Conv1D(c2, k2))\n",
    "    model.add(layers.BatchNormalization(momentum=m2))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.AveragePooling1D(p2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(y_dim, activation=\"softmax\"))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_test_conv_net(num_samples, architecture, epochs=EPOCHS):\n",
    "    sampled_df = sample_speeches(df, num_samples)\n",
    "    labels = sampled_df[\"person\"].astype(\"category\").cat.codes\n",
    "    data = np.stack(sampled_df.word_vectors.values)\n",
    "    model = build_conv_model(architecture, np.max(labels) + 1)\n",
    "    return train_and_test_net(model, data, labels, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Bereit für einen Testlauf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ARCHITECTURE = (64, 8, 8, 0.8, 64, 8, 8, 0.8)\n",
    "\n",
    "(acc, cm, hist) = train_and_test_conv_net(50, architecture=ARCHITECTURE)     \n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das Ergebnis ist nicht so berauschend, wenn man bedenkt, dass wir vorher schon eine durchschnittliche Genauigkeit von an die 70 Prozent erzielt hatten. Andererseits verwenden wir nun von jeder Rede nur die ersten 150 Token (beziehungsweise rund 100 Token, wenn man bedenkt, dass einige nicht in `w2v` enthalten sind). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Das Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ähnlich wie vorher werfen wir noch einen Blick auf die Streuung der Genauigkeit, die sich aus der zufälligen Auswahl an Trainings- und Testreden ergibt, und testen verschiedene Netz-Parameter. Das Experiment kann wieder eine Weile dauern. Wir haben es deswegen auskommentiert, bereits einmal durchgeführt und die Ergebnisse abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ARCHITECTURES = [(64, 8, 8, 0.99, 64, 8, 8, 0.99), (64, 8, 8, 0.8, 64, 8, 8, 0.8), \n",
    "                 (64, 8, 8, 0.6, 64, 8, 8, 0.6), (128, 8, 8, 0.7, 64, 8, 8, 0.7)]\n",
    "\n",
    "#(accuracies, losses) = neural_experiment(train_and_test_conv_net, num_samples=50, rounds=10, \n",
    "#                                         architectures=ARCHITECTURES, epochs=15)\n",
    "#save_results(accuracies, losses, \"conv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hier die Genauigkeiten und die Lernverläufe noch einmal visualisiert. Wie zuvor beobachten wir eine Überanpassung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "accuracies, losses = load_results(\"conv\")\n",
    "plot_net_accuracies(accuracies)\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Deep learning &mdash; und kein Erfolg? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Bei unseren Experimenten haben die fortgeschritteneren Techniken &mdash; also die neuronalen Netze und die Worteinbettungen &mdash; erstmal nicht die Vorhersagegenauigkeit geliefert, die wir uns vielleicht erhofft hatten. Aber wir sind auch von einer _sehr kleinen Datenmenge_ ausgegangen. Wenn Du Lust und Zeit hast und mit dem ungekürzten Datensatz experimentierst, wirst Du feststellen, dass sich die Vorhersagegenauigkeit wesentlich verbessert!\n",
    "\n",
    "_Wir hoffen, dass Dir das Experimentieren Spaß gemacht hat! Komm bald wieder zum codecentric.ai Bootcamp!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": "nlp_basics.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "399.533px",
    "width": "431px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
